{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepJetCore.DataCollection import DataCollection\n",
    "from DeepJetCore.compiled.c_trainDataGenerator import trainDataGenerator\n",
    "from DeepJetCore.customObjects import get_custom_objects\n",
    "# from DeepJetCore.training.gpuTools import DJCSetGPUs\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datastructures.TrainData_PF.TrainData_PF_graph at 0x7fc0f4374c70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define train_data as our data collection, read from djcdc file\n",
    "train_data = DataCollection(\"../data/train_data/dataCollection.djcdc\")\n",
    "\n",
    "train_data.dataclass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Condensation Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred (None, 200, 12)\n",
      "Mki (None, 200, None)\n",
      "kalpha (None, None)\n",
      "x_kalpha <unknown>\n",
      "Mki (None, 200, None)\n",
      "kalpha (None, None)\n",
      "x_kalpha <unknown>\n",
      "Mki (None, 200, None)\n",
      "kalpha (None, None)\n",
      "x_kalpha <unknown>\n",
      "Mki (None, 200, None)\n",
      "kalpha (None, None)\n",
      "x_kalpha <unknown>\n",
      "Mki (None, 200, None)\n",
      "kalpha (None, None)\n",
      "x_kalpha <unknown>\n",
      "Mki (None, 200, None)\n",
      "kalpha (None, None)\n",
      "x_kalpha <unknown>\n",
      "Mki (None, 200, None)\n",
      "kalpha (None, None)\n",
      "x_kalpha <unknown>\n",
      "Mki (None, 200, None)\n",
      "kalpha (None, None)\n",
      "x_kalpha <unknown>\n",
      "Mki (None, 200, None)\n",
      "kalpha (None, None)\n",
      "x_kalpha <unknown>\n",
      "Nobj (None,)\n",
      "Nobj (None,)\n",
      "../data/test_data/9.djctd\n",
      "../data/test_data/predictions/pred_9.djctd\n",
      "predicting  ../data/test_data/9.djctd\n",
      "batch size 1\n",
      "2275/2275 [==============================] - 34s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "### see https://github.com/DL4Jets/DeepJetCore/blob/master/bin/predict.py\n",
    "\n",
    "# prediction outputs\n",
    "outputs = []\n",
    "\n",
    "# get batchsize from train_data data collection\n",
    "batchsize = -1\n",
    "\n",
    "# DJCSetGPUs(args.gpu)\n",
    "\n",
    "# use as input model a previously (partially) trained model with same train_data data collection\n",
    "inputModel = \"../backup_results_full/KERAS_check_best_model_2.h5\"\n",
    "custom_objs = get_custom_objects()\n",
    "model = load_model(inputModel, custom_objects=custom_objs)\n",
    "\n",
    "# File Paths\n",
    "# file that we are generating predictions for\n",
    "inputfile = '../data/test_data/9.djctd'\n",
    "# same directory as input file\n",
    "outputDir = os.path.dirname(inputfile) + \"/predictions\"\n",
    "outputfilename = \"pred_\"+os.path.basename(inputfile)\n",
    "outputfile = outputDir + \"/\" + outputfilename\n",
    "print(inputfile)\n",
    "print(outputfile)\n",
    "\n",
    "# define td as the train_data data collection's dataclass (i.e. TrainData_PF_graph)\n",
    "td = train_data.dataclass()\n",
    "td.readFromFile(inputfile)\n",
    "\n",
    "# define training data generator class instance gen and set its variables\n",
    "print('predicting ',inputfile)\n",
    "gen = trainDataGenerator()\n",
    "if batchsize < 1:\n",
    "    batchsize = train_data.getBatchSize()\n",
    "print('batch size', batchsize)\n",
    "gen.setBatchSize(batchsize)\n",
    "gen.setSquaredElementsLimit(train_data.batch_uses_sum_of_squares)\n",
    "gen.setSkipTooLargeBatches(False)\n",
    "gen.setBuffer(td)\n",
    "\n",
    "# data generator function\n",
    "def genfunc():\n",
    "    while(not gen.isEmpty()):\n",
    "        d = gen.getBatch()\n",
    "        yield d.transferFeatureListToNumpy() , d.transferTruthListToNumpy()\n",
    "\n",
    "# predictions\n",
    "predicted = model.predict_generator(genfunc(),\n",
    "                                    steps=gen.getNBatches(),\n",
    "                                    max_queue_size=1,\n",
    "                                    use_multiprocessing=False,verbose=1)\n",
    "\n",
    "# dataclass contents: features, weights, truth\n",
    "# note: must convert these to numpy at the end of we mess things up\n",
    "features = td.transferFeatureListToNumpy()\n",
    "weights = td.transferWeightListToNumpy()\n",
    "truth = td.transferTruthListToNumpy()\n",
    "\n",
    "# actually convert to np array\n",
    "features_np = np.array(features)\n",
    "weights_np = np.array(weights)\n",
    "truth_np = np.array(truth)\n",
    "\n",
    "# clear variables\n",
    "td.clear()\n",
    "gen.clear()\n",
    "\n",
    "if not type(predicted) == list: #circumvent that keras return only an array if there is just one list item\n",
    "    predicted = [predicted]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputfile ../data/test_data/predictions/pred_9.djctd\n",
      "Condensation mask shape:  (2275, 200)\n",
      "Predictions shape:  (2275, 200, 12)\n",
      "all_idxs shape:  (200,)\n",
      "pred_E shape:  (2275, 200, 1)\n",
      "number of events:  2275\n",
      "particle property names: is_reco, reco_posx, reco_posy, reco_e, is_true, true_posx, true_posy, true_e, true_id, n_true\n",
      "event property names:  jet_mass_r_pu0.0, jet_mass_t_pu0.0, p_imbalance_x_r_pu0.0, p_imbalance_x_t_pu0.0,jet_mass_r_pu0.2, jet_mass_t_pu0.2, p_imbalance_x_r_pu0.2, p_imbalance_x_t_pu0.2,jet_mass_r_pu0.5, jet_mass_t_pu0.5, p_imbalance_x_r_pu0.5, p_imbalance_x_t_pu0.5,jet_mass_r_pu0.8, jet_mass_t_pu0.8, p_imbalance_x_r_pu0.8, p_imbalance_x_t_pu0.8,n_true, e_access_n\n",
      "all particles shape:  (9996, 10)\n",
      "all particles view:  [[  1.         -77.15816498  72.00442505 182.22807312   1.\n",
      "  -76.77187347  71.47179413 182.59280396   0.           8.        ]]\n",
      "all events shape:  (2275, 18)\n",
      "all events view:  [[9.6856830e+02 9.7319281e+02 2.4581887e+04 2.5175580e+04 9.6856830e+02\n",
      "  9.7319281e+02 2.4581887e+04 2.5175580e+04 9.1984814e+02 9.2431061e+02\n",
      "  1.9595596e+04 2.0187582e+04 7.0595172e+02 7.1137439e+02 2.3845320e+04\n",
      "  2.4378539e+04 8.0000000e+00 0.0000000e+00]]\n",
      "efficiency:  0.9879927956774065\n",
      "fake:  0.0002025111381125962\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from DeepJetCore.TrainData import TrainData\n",
    "\n",
    "from evaluation_tools import find_best_matching_truth_and_format, write_output_tree, determine_event_properties, write_event_output_tree\n",
    "from inference import make_particle_inference_dict,  collect_condensates\n",
    "\n",
    "\n",
    "allparticles=[]\n",
    "all_ev_prop=[]\n",
    "names=\"\"\n",
    "\n",
    "inputFileDir = \"../data/test_data/predictions\"\n",
    "inputFile = \"../data/test_data/predictions/outfiles.txt\"\n",
    "# should make the name dynamic... 9 for now since 9 is the test file\n",
    "outputFile = \"../data/test_data/predictions/condensates_9\"\n",
    "\n",
    "with open(inputFile) as file:\n",
    "    for inputfile in file:\n",
    "        inputfile = inputfile.replace('\\n', '')\n",
    "        inputfile = inputFileDir + '/' + inputfile\n",
    "        if len(inputfile)<1: continue\n",
    "\n",
    "        print('inputfile', inputfile)\n",
    "\n",
    "        td = TrainData()\n",
    "        td.readFromFile(inputfile)\n",
    "        indata = td.transferFeatureListToNumpy()\n",
    "        pred, feat, truth = indata[0],indata[1],indata[2]\n",
    "        del td\n",
    "\n",
    "        d = make_particle_inference_dict(pred, feat, truth)\n",
    "        condensate_mask = np.squeeze(collect_condensates(d, 0.1, 0.8),axis=2) #B x V x 1\n",
    "        \n",
    "        print(\"Condensation mask shape: \", condensate_mask.shape)\n",
    "#         print(\"Condensation mask view: \", condensate_mask[0:1])\n",
    "        \n",
    "        pred_E   = d['f_E']* d['p_E_corr']\n",
    "        pred_pos = d['f_pos'] + d['p_pos_offs']\n",
    "        calo_energy = None #not supported by data format..\n",
    "        #np.sum(d['f_E'][:,0:16*16,0],axis=-1)#calo energy\n",
    "        #loop over events here.. easier\n",
    "        \n",
    "        print(\"Predictions shape: \", pred.shape)\n",
    "#         print(\"Predictions view: \", pred[0:1])\n",
    "        nevents = pred.shape[0]\n",
    "        # basically np array from [0,200), over the range of V (which is 200)\n",
    "        all_idxs = np.array([i for i in range(pred.shape[1])])\n",
    "        print(\"all_idxs shape: \", all_idxs.shape)\n",
    "\n",
    "        #print('pred_pos',pred_pos.shape)\n",
    "        #print('all_idxs',all_idxs.shape)\n",
    "        \n",
    "        \n",
    "        print(\"pred_E shape: \", pred_E.shape)\n",
    "\n",
    "        # read more at: https://github.com/jkiesele/SOR/blob/master/modules/evaluation_tools.py\n",
    "        for event in range(nevents):\n",
    "            ev_pred_E    = pred_E[event][condensate_mask[event]>0][:,0]\n",
    "            ev_pred_pos  = pred_pos[event][condensate_mask[event]>0]\n",
    "            ev_truth = truth[event]\n",
    "            ob_idx = all_idxs[condensate_mask[event]>0]\n",
    "            \n",
    "            # returns returns p particle: is_reco, reco_posx, reco_posy, reco_e, is_true, true_posx, true_posy, true_e, true_id, n_true\n",
    "            eventparticles = find_best_matching_truth_and_format(ev_pred_pos, ev_pred_E, ob_idx, ev_truth)\n",
    "            allparticles.append(eventparticles)\n",
    "\n",
    "            ev_pro, names = determine_event_properties(eventparticles, None)\n",
    "            all_ev_prop.append(ev_pro)\n",
    "\n",
    "print(\"number of events: \", nevents)\n",
    "print(\"particle property names: is_reco, reco_posx, reco_posy, reco_e, is_true, true_posx, true_posy, true_e, true_id, n_true\")\n",
    "print(\"event property names: \", names)\n",
    "allparticles = np.concatenate(allparticles,axis=0)\n",
    "all_ev_prop = np.concatenate(all_ev_prop,axis=0)\n",
    "print(\"all particles shape: \", allparticles.shape)\n",
    "print(\"all particles view: \", allparticles[0:1])\n",
    "print(\"all events shape: \", all_ev_prop.shape)\n",
    "print(\"all events view: \", all_ev_prop[0:1])\n",
    "\n",
    "# efficiency is number of nonzero (is_reco * is_true) / is_true\n",
    "# so ratio of number that is_reco and is_true over number that is_true\n",
    "print('efficiency: ', float(np.count_nonzero( allparticles[:,0] *  allparticles[:,4]))/float( np.count_nonzero(allparticles[:,4] ) ))\n",
    "print('fake: ', float(np.count_nonzero( allparticles[:,0] *  (1.-allparticles[:,4])))/float( np.count_nonzero(allparticles[:,0] ) ))\n",
    "\n",
    "write_output_tree(allparticles, outputFile)\n",
    "write_event_output_tree(all_ev_prop, names, outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import make_particle_inference_dict, collect_condensates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condensate_mask shape:  (2275, 200)\n",
      "n_condensates\n",
      "n_condensates shape:  (2275, 1)\n",
      "n_true_particles\n",
      "n_true_particles shape:  (2275, 1)\n",
      "n_total_condensates:  9876\n",
      "n_total_true_particles:  9997.0\n",
      "fraction of right number:  0.9463736263736263\n"
     ]
    }
   ],
   "source": [
    "inputFile = \"../data/test_data/predictions/pred_9.djctd\"\n",
    "\n",
    "#just read the stored data\n",
    "td = TrainData()\n",
    "td.readFromFile(inputFile)\n",
    "indata = td.transferFeatureListToNumpy()\n",
    "pred, feat, truth = indata[0],indata[1],indata[2]\n",
    "del td\n",
    "\n",
    "d = make_particle_inference_dict(pred, feat , truth)\n",
    "# B x V x F\n",
    "pred_E   = d['f_E']* d['p_E_corr']\n",
    "pred_pos = d['f_pos'] + d['p_pos_offs']\n",
    "\n",
    "condensate_mask = collect_condensates(d, 0.1, 0.8) #B x V x 1\n",
    "condensate_mask = np.reshape(condensate_mask, [condensate_mask.shape[0],condensate_mask.shape[1]]) #B x V \n",
    "\n",
    "print(\"condensate_mask shape: \", condensate_mask.shape)\n",
    "\n",
    "n_condensates = np.sum(condensate_mask, axis=-1, keepdims=True) #B x 1\n",
    "\n",
    "# get number of true particles for each event by taking maximum of true object ids (t_objidx) among V dim\n",
    "n_true_particles = np.reshape(np.max(d['t_objidx'],axis=1)+1., [d['t_objidx'].shape[0],1])  #B x 1 x 1\n",
    "\n",
    "print(\"n_condensates\")\n",
    "print(\"n_condensates shape: \", n_condensates.shape)\n",
    "# print(n_condensates)\n",
    "print(\"n_true_particles\")\n",
    "print(\"n_true_particles shape: \", n_true_particles.shape)\n",
    "# print(n_true_particles)\n",
    "\n",
    "n_total_condensates = np.sum(n_condensates)\n",
    "print(\"n_total_condensates: \", n_total_condensates)\n",
    "\n",
    "n_total_true_particles = np.sum(n_true_particles)\n",
    "print(\"n_total_true_particles: \", n_total_true_particles)\n",
    "\n",
    "print('fraction of right number: ',float(np.sum(n_condensates==n_true_particles))/float(n_true_particles.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os.path as osp\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch_cluster import knn_graph\n",
    "\n",
    "from torch_geometric.nn import EdgeConv, NNConv\n",
    "from torch_geometric.nn.pool.edge_pool import EdgePooling\n",
    "\n",
    "from torch_geometric.utils import normalized_cut\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "from torch_geometric.utils.undirected import to_undirected\n",
    "from torch_geometric.nn import (graclus, max_pool, max_pool_x,\n",
    "                                global_mean_pool, global_max_pool,\n",
    "                                global_add_pool)\n",
    "\n",
    "# # something wrong with the package perhaps\n",
    "# from torch_geometric.nn import GravNetConv\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains the Estimator class implementation which provides\n",
    "code for doing the training of a PyTorch model.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import shutil \n",
    "import copy \n",
    "\n",
    "from torch_geometric.data import Batch      \n",
    "\n",
    "from Losses import particle_condensation_loss\n",
    "\n",
    "def logger(s):\n",
    "    \"\"\"Simple logger function which prints date/time\"\"\"\n",
    "    print(datetime.now(), s)\n",
    "\n",
    "class Estimator():\n",
    "    \"\"\"Estimator class\"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_func, opt='Adam',\n",
    "                 train_losses=None, valid_losses=None,\n",
    "                 cuda=False, l1=0.):\n",
    "\n",
    "        self.model = model\n",
    "        if cuda:\n",
    "            print(\"using CUDA...\")\n",
    "            self.model.cuda()\n",
    "        self.loss_func = loss_func\n",
    "        if opt == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        elif opt == 'SGD':\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters())\n",
    "\n",
    "        self.train_losses = train_losses if train_losses is not None else []\n",
    "        self.valid_losses = valid_losses if valid_losses is not None else []\n",
    "        self.l1 = l1\n",
    "\n",
    "        logger('Model: \\n%s' % model)\n",
    "        logger('Parameters: %i' %\n",
    "               sum(param.numel() for param in model.parameters()))\n",
    "\n",
    "    def l1_penalty(self, arr):\n",
    "        return torch.abs(arr).sum()\n",
    "        \n",
    "    def training_step(self, inputs, targets):\n",
    "        '''\n",
    "        Applies single optimization step on batch\n",
    "        '''\n",
    "                \n",
    "        # define batch input and target tensors\n",
    "        batch_input_tensor = torch.from_numpy(np.array(inputs[0]))\n",
    "        batch_target_tensor = torch.from_numpy(np.array(targets[0]))\n",
    "        \n",
    "        d = make_particle_inference_dict(None, inputs[0], targets[0])\n",
    "        \n",
    "        nevents = d['t_E'].shape[0]\n",
    "        idxs_dominant = np.argmax(d['t_E'][:,:,0], axis=1)\n",
    "        t_particle_id_list = []\n",
    "        for i in range(0, nevents):\n",
    "            idx_dominant = idxs_dominant[i]\n",
    "            dom_ID = int(d['t_ID'][i][:,0][idx_dominant])\n",
    "            t_particle_id_list.append(dom_ID)\n",
    "        t_particle_id_np = np.array(t_particle_id_list)\n",
    "#         print(t_particle_id_np)\n",
    "        print(np.sum(t_particle_id_np) / nevents)\n",
    "        target_particle_id_tensor = torch.from_numpy(t_particle_id_np)\n",
    "        \n",
    "        # t_maskmask = torch.from_numpy(d['t_mask'][:,:,0])\n",
    "        \n",
    "\n",
    "        # kinda jank... get dimensions\n",
    "        batch_size = len(inputs[0]) # same as nevents\n",
    "        num_nodes = len(inputs[0][0]) # should be 200?\n",
    "        num_features = len(inputs[0][0][0]) # should be 6\n",
    "        num_truth_vals = len(targets[0][0][0]) # should be 11\n",
    "        \n",
    "#         # number of true particles\n",
    "#         n_true_particles = np.reshape(np.max(d['t_objidx'],axis=1)+1., [d['t_objidx'].shape[0],1])  #B x 1 x 1\n",
    "#         n_total_true_particles = int(np.sum(n_true_particles))\n",
    "#         print(\"number of total true particles: \", n_total_true_particles)\n",
    "        \n",
    "        # flatten from 3D to 2D by combining B and V dimmensions\n",
    "        x_tensor_flat = batch_input_tensor.view(-1, num_features)\n",
    "        y_tensor_flat = batch_target_tensor.view(-1, num_truth_vals)\n",
    "        \n",
    "\n",
    "        # tensor keeping track of which batch each entry is from, after flattening\n",
    "        batch_list = [i for i in range(batch_size) for j in range(num_nodes)]\n",
    "        batch_np = np.array(batch_list)\n",
    "        batch_tensor = torch.from_numpy(batch_np)\n",
    "        \n",
    "        # cartesian coordinate tensor from x_tensor_flat\n",
    "        pos_tensor = x_tensor_flat.narrow(1,1,3)\n",
    "\n",
    "        # put data into a batch structure\n",
    "        data = Batch(batch=batch_tensor, x=x_tensor_flat, edge_index=None, edge_attr=None, pos=pos_tensor)\n",
    "        \n",
    "        print(\"data x shape: \", data.x.size())\n",
    "        # modify data attributes - zero-suppressed\n",
    "        # mask based on energy - keep only sensors/cells with nonzero energy hit\n",
    "        print(\"mask shape: \", y_tensor_flat[:,0].size())\n",
    "        print(y_tensor_flat[:,0])\n",
    "        mask = (y_tensor_flat[:,0] > 0.).squeeze()\n",
    "        data.x = data.x[mask]\n",
    "        data.pos = data.pos[mask,:]\n",
    "        data.batch = data.batch[mask.squeeze()]\n",
    "\n",
    "#         print(\"data x shape: \", data.x.size())\n",
    "#         print(\"data pos shape: \", data.pos.size())\n",
    "#         print(\"data batch shape: \", data.batch.size())\n",
    "        \n",
    "        print(\"Applying single optimization step on batch\")\n",
    "        self.model.zero_grad()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # call the forward step of our model\n",
    "        outputs = self.model(data)\n",
    "        \n",
    "#         outputs_np = np.expand_dims(outputs.detach().numpy(), axis=0)\n",
    "        outputs_np = outputs.detach().numpy()\n",
    "    \n",
    "        print(outputs_np)\n",
    "        print(outputs.max(1)[1])\n",
    "        \n",
    "        # negative log likelihood loss\n",
    "        loss = F.nll_loss(outputs, target_particle_id_tensor)\n",
    "\n",
    "#         loss = particle_condensation_loss(batch_target_np, outputs_np)\n",
    "    \n",
    "        print(\"training step loss: \", loss)\n",
    "        \n",
    "        # backwards step\n",
    "        loss.backward()\n",
    "\n",
    "        #print(torch.unique(torch.argmax(result, dim=-1)))\n",
    "        #print(torch.unique(data.y))\n",
    "        self.optimizer.step()\n",
    "\n",
    "#         scheduler.batch_step()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def save_checkpoint(self, state, is_best, filename='checkpoint.pt'):\n",
    "        directory = os.path.dirname(filename)\n",
    "        try:\n",
    "            os.stat(directory)\n",
    "        except:\n",
    "            os.mkdir(directory)\n",
    "        torch.save(state, filename)\n",
    "        if is_best:\n",
    "            bestfilename = directory+'/model_best.pt'\n",
    "            shutil.copyfile(filename, bestfilename)\n",
    "            \n",
    "    def load_checkpoint(self, filename='checkpoint.pt'):\n",
    "        checkpoint = torch.load(filename)\n",
    "        self.model.load_state_dict(checkpoint['state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.valid_losses = checkpoint['valid_losses']\n",
    "        self.train_losses = checkpoint['train_losses']\n",
    "        \n",
    "    def load_weights(self, filename='checkpoint.pt'):\n",
    "        checkpoint = torch.load(filename)\n",
    "        old_model = copy.deepcopy(self.model)\n",
    "        old_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        def set_masked_data(new_layer, old_layer):\n",
    "            if new_layer.mask_flag:\n",
    "                new_layer.weight.data = old_layer.weight.data * old_layer.mask.data\n",
    "            else:\n",
    "                new_layer.weight.data = old_layer.weight.data\n",
    "              \n",
    "        set_masked_data(self.model.edge_network.network[0], old_model.edge_network.network[0])\n",
    "        set_masked_data(self.model.edge_network.network[2], old_model.edge_network.network[2])\n",
    "        set_masked_data(self.model.node_network.network[0], old_model.node_network.network[0])\n",
    "        set_masked_data(self.model.node_network.network[2], old_model.node_network.network[2])       \n",
    "    \n",
    "    def fit_gen(self, train_generator, n_batches=1, n_epochs=1,\n",
    "                valid_generator=None, n_valid_batches=1, verbose=0, \n",
    "                filename='checkpoint.pt'):\n",
    "        \"\"\"Runs batch training for a number of specified epochs.\"\"\"\n",
    "        epoch_start = len(self.train_losses)\n",
    "        epoch_end = epoch_start + n_epochs\n",
    "        if len(self.valid_losses) > 0:\n",
    "            best_valid_loss = self.valid_losses[-1]\n",
    "        else:\n",
    "            best_valid_loss = 99999999\n",
    "        for i in range(epoch_start, epoch_end):\n",
    "            logger('Epoch %i' % i)\n",
    "            start_time = timer()\n",
    "            sum_loss = 0\n",
    "\n",
    "            # Train the model\n",
    "            print(\"training model...\")\n",
    "            self.model.train()\n",
    "            \n",
    "            print(\"computing losses for {} batches...\".format(n_batches))\n",
    "            for j in range(n_batches):\n",
    "                batch_input, batch_target = next(train_generator)\n",
    "                batch_loss = (self.training_step(batch_input, batch_target)\n",
    "                              .cpu().data.item())\n",
    "                print(\"batch {} out of {}\".format(j+1, n_batches))\n",
    "                print(\"batch_loss: \", batch_loss)\n",
    "                sum_loss += batch_loss\n",
    "                print(\"sum loss: \", sum_loss)\n",
    "                if verbose > 0:\n",
    "                    logger('  Batch %i loss %f' % (j, batch_loss))\n",
    "            print(\"finished cycle\")\n",
    "            end_time = timer()\n",
    "            avg_loss = sum_loss / n_batches\n",
    "            self.train_losses.append(avg_loss)\n",
    "            logger('  training loss %.3g time %gs' %\n",
    "                   (avg_loss, (end_time - start_time)))\n",
    "\n",
    "            # TODO: adapt this to new data scheme\n",
    "            with torch.no_grad():\n",
    "                # Evaluate the model on the validation set\n",
    "                if (valid_generator is not None) and (n_valid_batches > 0):\n",
    "                    self.model.eval()\n",
    "                    valid_loss = 0\n",
    "                    for j in range(n_valid_batches):\n",
    "                        valid_input, valid_target = next(valid_generator)\n",
    "                        valid_loss += (self.loss_func(self.model(valid_input), valid_target)\n",
    "                                       .cpu().data.item())\n",
    "                    valid_loss = valid_loss / n_valid_batches\n",
    "                    self.valid_losses.append(valid_loss)\n",
    "                    logger('  validate loss %.3g' % valid_loss)\n",
    "                \n",
    "                    #Save model checkpoint - modified\n",
    "                    logger(' save checkpoint') \n",
    "                    is_best = valid_loss < best_valid_loss\n",
    "                    best_valid_loss = min(valid_loss, best_valid_loss)\n",
    "                    self.save_checkpoint({\n",
    "                        'epoch': i + 1,\n",
    "                        'state_dict': self.model.state_dict(),\n",
    "                        'best_valid_loss': best_valid_loss,\n",
    "                        'valid_losses': self.valid_losses,\n",
    "                        'train_losses': self.train_losses,\n",
    "                        'optimizer' : self.optimizer.state_dict(),\n",
    "                    }, is_best, filename=filename)\n",
    "\n",
    "    def predict(self, generator, n_batches, concat=True):\n",
    "        with torch.no_grad():  \n",
    "            self.model.eval()\n",
    "            outputs = []\n",
    "            for j in range(n_batches):\n",
    "                test_input, test_target = next(generator)\n",
    "                outputs.append(self.model(test_input))\n",
    "            if concat:\n",
    "                outputs = torch.cat(outputs)\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRN with some edits to make it work for object condensation and current dataset\n",
    "\n",
    "transform = T.Cartesian(cat=False)\n",
    "\n",
    "def normalized_cut_2d(edge_index, pos):\n",
    "    row, col = edge_index\n",
    "    edge_attr = torch.norm(pos[row] - pos[col], p=2, dim=1)\n",
    "    return normalized_cut(edge_index, edge_attr, num_nodes=pos.size(0))\n",
    "\n",
    "class DynamicReductionNetwork(nn.Module):\n",
    "    # This model iteratively contracts nearest neighbour graphs \n",
    "    # until there is one output node.\n",
    "    # The latent space trained to group useful features at each level\n",
    "    # of aggregration.\n",
    "    # This allows single quantities to be regressed from complex point counts\n",
    "    # in a location and orientation invariant way.\n",
    "    # One encoding layer is used to abstract away the input features.\n",
    "    def __init__(self, input_dim=5, hidden_dim=64, output_dim=1, k=16, aggr='add',\n",
    "                 norm=torch.tensor([1./300.,1./200.,1./200.,1./100.,1./100.,1./100.])):\n",
    "        super(DynamicReductionNetwork, self).__init__()\n",
    "\n",
    "        self.datanorm = nn.Parameter(norm)\n",
    "        \n",
    "        self.k = k\n",
    "        start_width = 2 * hidden_dim\n",
    "        middle_width = 3 * hidden_dim // 2\n",
    "\n",
    "        self.inputnet =  nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim//2),            \n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "        )        \n",
    "        convnn1 = nn.Sequential(nn.Linear(start_width, middle_width),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(middle_width, hidden_dim),                                             \n",
    "                                nn.ELU()\n",
    "                                )\n",
    "        convnn2 = nn.Sequential(nn.Linear(start_width, middle_width),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(middle_width, hidden_dim),                                             \n",
    "                                nn.ELU()\n",
    "                                )                \n",
    "        \n",
    "        # The edge convolutional operator from the “Dynamic Graph CNN for Learning on Point Clouds” paper\n",
    "        self.edgeconv1 = EdgeConv(nn=convnn1, aggr=aggr)\n",
    "        self.edgeconv2 = EdgeConv(nn=convnn2, aggr=aggr)\n",
    "        \n",
    "#         # object condensation layers... need to convert to pytorch\n",
    "#         self.condensation = nn.Sequential(\n",
    "#             nn.BatchNorm2d(momentum=0.6)\n",
    "#             feat=[data.x]\n",
    "\n",
    "#             for i in range(2): # 6\n",
    "#                 #add global exchange and another dense here\n",
    "#                 x = GlobalExchange()(x)\n",
    "#                 x = Dense(64, activation='elu')(x)\n",
    "#                 x = Dense(64, activation='elu')(x)\n",
    "#                 x = BatchNormalization(momentum=0.6)(x)\n",
    "#                 x = Dense(64, activation='elu')(x)\n",
    "#                 x = nn.GravNetConv(n_neighbours=10, \n",
    "#                          n_dimensions=4, \n",
    "#                          n_filters=128, \n",
    "#                          n_propagate=64)(x)\n",
    "#                 x = BatchNormalization(momentum=0.6)(x)\n",
    "#                 feat.append(Dense(32, activation='elu')(x))\n",
    "\n",
    "#             x = Concatenate()(feat)\n",
    "#             x = Dense(64, activation='elu')(x)\n",
    "#         )\n",
    "        \n",
    "        self.output = nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n",
    "                                    nn.ELU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim//2),\n",
    "                                    nn.ELU(),                                    \n",
    "                                    nn.Linear(hidden_dim//2, output_dim))\n",
    "        \n",
    "        \n",
    "    def forward(self, data):\n",
    "        print(\"x shape: \", data.x.size())\n",
    "        data.x = self.datanorm * data.x\n",
    "        data.x = self.inputnet(data.x)\n",
    "        print(\"x shape: \", data.x.size())\n",
    "        data.edge_index = to_undirected(knn_graph(data.x, self.k, data.batch, loop=False, flow=self.edgeconv1.flow))\n",
    "        data.x = self.edgeconv1(data.x, data.edge_index)\n",
    "        \n",
    "        print(\"x shape: \", data.x.size())\n",
    "            \n",
    "        weight = normalized_cut_2d(data.edge_index, data.x)\n",
    "        \n",
    "        # replace with condensation-based clustering ?\n",
    "        print(\"clustering to dimension: \", data.x.size(0))\n",
    "        cluster = graclus(data.edge_index, weight, data.x.size(0))\n",
    "        print(\"x shape: \", data.x.size())\n",
    "\n",
    "        data.edge_attr = None\n",
    "        data = max_pool(cluster, data)\n",
    "        \n",
    "        print(\"x shape: \", data.x.size())\n",
    "        \n",
    "        data.edge_index = to_undirected(knn_graph(data.x, self.k, data.batch, loop=False, flow=self.edgeconv2.flow))\n",
    "        data.x = self.edgeconv2(data.x, data.edge_index)\n",
    "        \n",
    "        print(\"x shape: \", data.x.size())\n",
    "                \n",
    "        weight = normalized_cut_2d(data.edge_index, data.x)\n",
    "        \n",
    "        # replace with condensation-based clustering ?\n",
    "        print(\"clustering to dimension: \", data.x.size(0))\n",
    "        cluster = graclus(data.edge_index, weight, data.x.size(0))\n",
    "        print(\"x shape: \", data.x.size())\n",
    "        \n",
    "        x, batch = max_pool_x(cluster, data.x, data.batch)\n",
    "        print(\"x shape: \", x.size())\n",
    "#         # it seems like the batch tensor (each batch corresponds to an event) at this point associates each\n",
    "#         # point in x with the event it is from. It looks almost exactly like the condensation eventparticles\n",
    "#         print(batch.size())\n",
    "#         print(batch[0:40])\n",
    "        x = global_max_pool(x, batch)\n",
    "        print(\"x shape: \", x.size())\n",
    "                \n",
    "        x = self.output(x).squeeze(-1)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-25 02:06:22.140637 Model: \n",
      "DynamicReductionNetwork(\n",
      "  (inputnet): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=10, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "  )\n",
      "  (edgeconv1): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=40, out_features=30, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "  ))\n",
      "  (edgeconv2): EdgeConv(nn=Sequential(\n",
      "    (0): Linear(in_features=40, out_features=30, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "  ))\n",
      "  (output): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Linear(in_features=10, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "2020-06-25 02:06:22.140801 Parameters: 5178\n",
      "2020-06-25 02:06:22.141081 Epoch 0\n",
      "training model...\n",
      "computing losses for 599 batches...\n",
      "0.506\n",
      "data x shape:  torch.Size([100000, 6])\n",
      "mask shape:  torch.Size([100000])\n",
      "tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Applying single optimization step on batch\n",
      "x shape:  torch.Size([7796, 6])\n",
      "x shape:  torch.Size([7796, 20])\n",
      "x shape:  torch.Size([7796, 20])\n",
      "clustering to dimension:  7796\n",
      "x shape:  torch.Size([7796, 20])\n",
      "x shape:  torch.Size([4094, 20])\n",
      "x shape:  torch.Size([4094, 20])\n",
      "clustering to dimension:  4094\n",
      "x shape:  torch.Size([4094, 20])\n",
      "x shape:  torch.Size([2154, 20])\n",
      "x shape:  torch.Size([500, 20])\n",
      "[[-0.44580847 -1.8370795  -4.296302   ... -6.5536532  -9.642312\n",
      "  -4.840708  ]\n",
      " [-1.139405   -1.6735898  -3.0490828  ... -3.9967458  -5.7786236\n",
      "  -3.1891751 ]\n",
      " [-0.36713788 -2.064638   -4.2607875  ... -6.6481576  -9.723223\n",
      "  -4.850808  ]\n",
      " ...\n",
      " [-1.3801602  -1.6904713  -2.8726172  ... -3.6328125  -5.1701746\n",
      "  -2.981172  ]\n",
      " [-1.356611   -1.6760867  -2.912781   ... -3.7298994  -5.279826\n",
      "  -3.0458746 ]\n",
      " [-0.6206332  -1.7038546  -3.756156   ... -5.661441   -7.898033\n",
      "  -4.1135774 ]]\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "training step loss:  tensor(1.5531, grad_fn=<NllLossBackward>)\n",
      "batch 1 out of 599\n",
      "batch_loss:  1.5531450510025024\n",
      "sum loss:  1.5531450510025024\n",
      "0.492\n",
      "data x shape:  torch.Size([100000, 6])\n",
      "mask shape:  torch.Size([100000])\n",
      "tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Applying single optimization step on batch\n",
      "x shape:  torch.Size([7697, 6])\n",
      "x shape:  torch.Size([7697, 20])\n",
      "x shape:  torch.Size([7697, 20])\n",
      "clustering to dimension:  7697\n",
      "x shape:  torch.Size([7697, 20])\n",
      "x shape:  torch.Size([4042, 20])\n",
      "x shape:  torch.Size([4042, 20])\n",
      "clustering to dimension:  4042\n",
      "x shape:  torch.Size([4042, 20])\n",
      "x shape:  torch.Size([2136, 20])\n",
      "x shape:  torch.Size([500, 20])\n",
      "[[-1.0142521 -1.0053879 -3.7470365 ... -5.375731  -7.5918784 -3.6512127]\n",
      " [-1.3907175 -1.225023  -3.060275  ... -4.1081457 -5.6656055 -3.0302875]\n",
      " [-2.1899679 -1.7002285 -2.588214  ... -3.0009904 -3.5791826 -2.5911326]\n",
      " ...\n",
      " [-1.0197753 -1.0318409 -3.746121  ... -5.404821  -7.6037674 -3.7329106]\n",
      " [-2.548195  -1.9339702 -2.4451687 ... -2.7524123 -2.8379364 -2.4825106]\n",
      " [-1.7600346 -1.4736694 -2.7916718 ... -3.4174652 -4.525705  -2.7730646]]\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1])\n",
      "training step loss:  tensor(1.3347, grad_fn=<NllLossBackward>)\n",
      "batch 2 out of 599\n",
      "batch_loss:  1.3347480297088623\n",
      "sum loss:  2.8878930807113647\n",
      "0.504\n",
      "data x shape:  torch.Size([100000, 6])\n",
      "mask shape:  torch.Size([100000])\n",
      "tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Applying single optimization step on batch\n",
      "x shape:  torch.Size([7432, 6])\n",
      "x shape:  torch.Size([7432, 20])\n",
      "x shape:  torch.Size([7432, 20])\n",
      "clustering to dimension:  7432\n",
      "x shape:  torch.Size([7432, 20])\n",
      "x shape:  torch.Size([3916, 20])\n",
      "x shape:  torch.Size([3916, 20])\n",
      "clustering to dimension:  3916\n",
      "x shape:  torch.Size([3916, 20])\n",
      "x shape:  torch.Size([2066, 20])\n",
      "x shape:  torch.Size([500, 20])\n",
      "[[-0.8665981  -1.1940858  -3.670133   ... -5.1108985  -7.440835\n",
      "  -3.6699219 ]\n",
      " [-2.4012704  -1.7959851  -2.490133   ... -2.8604922  -3.0990937\n",
      "  -2.501037  ]\n",
      " [-2.6496854  -1.9776751  -2.3779082  ... -2.733171   -2.6443918\n",
      "  -2.4412425 ]\n",
      " ...\n",
      " [-1.0091994  -0.7494839  -4.4320426  ... -6.2617397  -8.639369\n",
      "  -4.1481395 ]\n",
      " [-0.95871127 -0.70822036 -4.7853165  ... -6.9622946  -9.618627\n",
      "  -4.576225  ]\n",
      " [-2.4274974  -1.8121462  -2.4758997  ... -2.8479152  -3.053989\n",
      "  -2.496358  ]]\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "training step loss:  tensor(1.2832, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3 out of 599\n",
      "batch_loss:  1.2832014560699463\n",
      "sum loss:  4.171094536781311\n",
      "0.522\n",
      "data x shape:  torch.Size([100000, 6])\n",
      "mask shape:  torch.Size([100000])\n",
      "tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Applying single optimization step on batch\n",
      "x shape:  torch.Size([7647, 6])\n",
      "x shape:  torch.Size([7647, 20])\n",
      "x shape:  torch.Size([7647, 20])\n",
      "clustering to dimension:  7647\n",
      "x shape:  torch.Size([7647, 20])\n",
      "x shape:  torch.Size([4005, 20])\n",
      "x shape:  torch.Size([4005, 20])\n",
      "clustering to dimension:  4005\n",
      "x shape:  torch.Size([4005, 20])\n",
      "x shape:  torch.Size([2116, 20])\n",
      "x shape:  torch.Size([500, 20])\n",
      "[[ -2.7465932   -2.0098693   -2.3356721  ...  -2.7221      -2.4917505\n",
      "   -2.417972  ]\n",
      " [ -1.0150844   -0.7681929   -4.1528263  ...  -5.927483    -8.36865\n",
      "   -3.998241  ]\n",
      " [ -2.6777873   -1.9754832   -2.3615     ...  -2.737598    -2.5989676\n",
      "   -2.4338055 ]\n",
      " ...\n",
      " [ -0.8662411   -0.740862    -4.8050795  ...  -7.0427547   -9.932113\n",
      "   -4.647725  ]\n",
      " [ -0.95889544  -0.7513876   -4.458355   ...  -6.352718    -8.897056\n",
      "   -4.2822256 ]\n",
      " [ -0.86029553  -0.6253698   -6.090616   ...  -9.2697525  -12.972841\n",
      "   -5.9969893 ]]\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "training step loss:  tensor(1.1754, grad_fn=<NllLossBackward>)\n",
      "batch 4 out of 599\n",
      "batch_loss:  1.1754279136657715\n",
      "sum loss:  5.3465224504470825\n",
      "0.506\n",
      "data x shape:  torch.Size([100000, 6])\n",
      "mask shape:  torch.Size([100000])\n",
      "tensor([1., 1., 1.,  ..., 0., 0., 0.])\n",
      "Applying single optimization step on batch\n",
      "x shape:  torch.Size([7735, 6])\n",
      "x shape:  torch.Size([7735, 20])\n",
      "x shape:  torch.Size([7735, 20])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-ec846204b3a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#               filename='test.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mestim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'checkpoint.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-ce7aa47b473a>\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, train_generator, n_batches, n_epochs, valid_generator, n_valid_batches, verbose, filename)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 batch_loss = (self.training_step(batch_input, batch_target)\n\u001b[0m\u001b[1;32m    205\u001b[0m                               .cpu().data.item())\n\u001b[1;32m    206\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch {} out of {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-ce7aa47b473a>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# call the forward step of our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m#         outputs_np = np.expand_dims(outputs.detach().numpy(), axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-ecf3a97f6fdf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_cut_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# replace with condensation-based clustering ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-ecf3a97f6fdf>\u001b[0m in \u001b[0;36mnormalized_cut_2d\u001b[0;34m(edge_index, pos)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalized_cut_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0medge_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnormalized_cut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_data = DataCollection(\"../data/train_data/example_dataCollection.djcdc\")\n",
    "\n",
    "# splits off 10% of the training dataset for validation. Can be used in the same way as train_data\n",
    "val_data = train_data.split(0.9) \n",
    "\n",
    "# Set the batch size. \n",
    "# If the data is ragged in dimension 1 (see convert options), \n",
    "# then this is the maximum number of elements per batch, which could be distributed differently\n",
    "# to individual examples. E.g., if the first example has 50 elements, the second 48, and the third 30,\n",
    "# and the batch size is set to 100, it would return the first two examples (in total 99 elements) in \n",
    "# the first batch etc. This is helpful to avoid out-of-memory errors during training\n",
    "\n",
    "train_data.setBatchSize(500)         \n",
    "    \n",
    "# prepare the generator\n",
    "train_data.invokeGenerator()\n",
    "        \n",
    "train_data.generator.shuffleFilelist()\n",
    "# train_data.generator.prepareNextEpoch()\n",
    "\n",
    "# this number can differ from epoch to epoch for ragged data!\n",
    "n_train_batches = train_data.generator.getNBatches()\n",
    "\n",
    "\n",
    "# Model config\n",
    "hidden_dim = 20\n",
    "# n_iters = 4\n",
    "n_features = 6\n",
    "n_outputs = 12\n",
    "# number of nearest neighbors?\n",
    "k = 10\n",
    "\n",
    "n_epochs = 10 #100\n",
    "# valid_frac = 0.1\n",
    "# test_frac = 0\n",
    "\n",
    "# def __init__(self, input_dim=5, hidden_dim=64, output_dim=1, k=16, aggr='add',\n",
    "model = DynamicReductionNetwork(input_dim=n_features, hidden_dim=hidden_dim, output_dim=n_outputs, k=k)\n",
    "# loss function\n",
    "loss_func = nn.BCELoss()\n",
    "# estim = Estimator(model, loss_func=loss_func, cuda=cuda, l1 = 0)\n",
    "estim = Estimator(model, loss_func=loss_func, cuda=False, l1 = 0)\n",
    "\n",
    "# training data generator\n",
    "gen = train_data.generator\n",
    "\n",
    "def genfunc():\n",
    "    while(not gen.isEmpty()):\n",
    "        d = gen.getBatch()\n",
    "        yield d.transferFeatureListToNumpy() , d.transferTruthListToNumpy()\n",
    "\n",
    "# # test batch generator output\n",
    "# batch_input, batch_target = next(genfunc())\n",
    "# print(batch_input)\n",
    "# print(batch_target)\n",
    "\n",
    "# fit estimator with data generator(s)\n",
    "# note there are more arguments: \n",
    "# estim.fit_gen(train_batcher, n_batches=n_train_batches, n_epochs=n_epochs,\n",
    "#               valid_generator=valid_batcher, n_valid_batches=n_valid_batches, \n",
    "#               filename='test.pt')\n",
    "\n",
    "estim.fit_gen(genfunc(), n_batches=n_train_batches, n_epochs=n_epochs, filename='checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "plt.figure()\n",
    "plt.plot(estim.train_losses, label='training set')\n",
    "plt.plot(estim.valid_losses, label='validation set')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.xlim(195,300)\n",
    "#plt.ylim(.06,.08)\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
