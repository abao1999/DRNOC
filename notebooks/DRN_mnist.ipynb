{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch_cluster import knn_graph\n",
    "\n",
    "from torch_geometric.nn import EdgeConv, NNConv\n",
    "from torch_geometric.nn.pool.edge_pool import EdgePooling\n",
    "\n",
    "from torch_geometric.utils import normalized_cut\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "from torch_geometric.utils.undirected import to_undirected\n",
    "from torch_geometric.nn import (graclus, max_pool, max_pool_x,\n",
    "                                global_mean_pool, global_max_pool,\n",
    "                                global_add_pool)\n",
    "\n",
    "transform = T.Cartesian(cat=False)\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import math\n",
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceMaxLROnRestart:\n",
    "    def __init__(self, ratio=0.75):\n",
    "        self.ratio = ratio\n",
    "        \n",
    "        def __call__(self, eta_min, eta_max):\n",
    "            return eta_min, eta_max * self.ratio\n",
    "        \n",
    "        \n",
    "class ExpReduceMaxLROnIteration:\n",
    "    def __init__(self, gamma=1):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def __call__(self, eta_min, eta_max, iterations):\n",
    "        return eta_min, eta_max * self.gamma ** iterations\n",
    "\n",
    "\n",
    "class CosinePolicy:\n",
    "    def __call__(self, t_cur, restart_period):\n",
    "        return 0.5 * (1. + math.cos(math.pi *\n",
    "                                    (t_cur / restart_period)))\n",
    "    \n",
    "    \n",
    "class ArccosinePolicy:\n",
    "    def __call__(self, t_cur, restart_period):\n",
    "        return (math.acos(max(-1, min(1, 2 * t_cur\n",
    "                                      / restart_period - 1))) / math.pi)\n",
    "    \n",
    "    \n",
    "class TriangularPolicy:\n",
    "    def __init__(self, triangular_step=0.5):\n",
    "        self.triangular_step = triangular_step\n",
    "        \n",
    "    def __call__(self, t_cur, restart_period):\n",
    "        inflection_point = self.triangular_step * restart_period\n",
    "        point_of_triangle = (t_cur / inflection_point\n",
    "                             if t_cur < inflection_point\n",
    "                             else 1.0 - (t_cur - inflection_point)\n",
    "                             / (restart_period - inflection_point))\n",
    "        return point_of_triangle\n",
    "    \n",
    "    \n",
    "class CyclicLRWithRestarts(_LRScheduler):\n",
    "    \"\"\"Decays learning rate with cosine annealing, normalizes weight decay\n",
    "    hyperparameter value, implements restarts.\n",
    "    https://arxiv.org/abs/1711.05101\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        batch_size: minibatch size\n",
    "        epoch_size: training samples per epoch\n",
    "        restart_period: epoch count in the first restart period\n",
    "        t_mult: multiplication factor by which the next restart period will expand/shrink\n",
    "        policy: [\"cosine\", \"arccosine\", \"triangular\", \"triangular2\", \"exp_range\"]\n",
    "        min_lr: minimum allowed learning rate\n",
    "        verbose: print a message on every restart\n",
    "        gamma: exponent used in \"exp_range\" policy\n",
    "        eta_on_restart_cb: callback executed on every restart, adjusts max or min lr\n",
    "        eta_on_iteration_cb: callback executed on every iteration, adjusts max or min lr\n",
    "        triangular_step: adjusts ratio of increasing/decreasing phases for triangular policy\n",
    "    Example:\n",
    "        >>> scheduler = CyclicLRWithRestarts(optimizer, 32, 1024, restart_period=5, t_mult=1.2)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     scheduler.step()\n",
    "        >>>     train(...)\n",
    "        >>>         ...\n",
    "        >>>         optimizer.zero_grad()\n",
    "        >>>         loss.backward()\n",
    "        >>>         optimizer.step()\n",
    "        >>>         scheduler.batch_step()\n",
    "        >>>     validate(...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, batch_size, epoch_size, restart_period=100,\n",
    "                 t_mult=2, last_epoch=-1, verbose=False,\n",
    "                 policy=\"cosine\", policy_fn=None, min_lr=1e-7,\n",
    "                 eta_on_restart_cb=None, eta_on_iteration_cb=None,\n",
    "                 gamma=1.0, triangular_step=0.5):\n",
    "        \n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        if last_epoch == -1:\n",
    "            for group in optimizer.param_groups:\n",
    "                group.setdefault('initial_lr', group['lr'])\n",
    "                group.setdefault('minimum_lr', min_lr)\n",
    "        else:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if 'initial_lr' not in group:\n",
    "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
    "                                   \"in param_groups[{}] when resuming an\"\n",
    "                                   \" optimizer\".format(i))\n",
    "                \n",
    "        self.base_lrs = [group['initial_lr'] for group\n",
    "                         in optimizer.param_groups]\n",
    "        \n",
    "        self.min_lrs = [group['minimum_lr'] for group\n",
    "                        in optimizer.param_groups]\n",
    "        \n",
    "        self.base_weight_decays = [group['weight_decay'] for group\n",
    "                                   in optimizer.param_groups]\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.eta_on_restart_cb = eta_on_restart_cb\n",
    "        self.eta_on_iteration_cb = eta_on_iteration_cb\n",
    "        if policy_fn is not None:\n",
    "            self.policy_fn = policy_fn\n",
    "        elif self.policy == \"cosine\":\n",
    "            self.policy_fn = CosinePolicy()\n",
    "        elif self.policy == \"arccosine\":\n",
    "            self.policy_fn = ArccosinePolicy()\n",
    "        elif self.policy == \"triangular\":\n",
    "            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n",
    "        elif self.policy == \"triangular2\":\n",
    "            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n",
    "            self.eta_on_restart_cb = ReduceMaxLROnRestart(ratio=0.5)\n",
    "        elif self.policy == \"exp_range\":\n",
    "            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n",
    "            self.eta_on_iteration_cb = ExpReduceMaxLROnIteration(gamma=gamma)\n",
    "            \n",
    "        self.last_epoch = last_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_size = epoch_size\n",
    "        \n",
    "        self.iteration = 0\n",
    "        self.total_iterations = 0\n",
    "        \n",
    "        self.t_mult = t_mult\n",
    "        self.verbose = verbose\n",
    "        self.restart_period = math.ceil(restart_period)\n",
    "        self.restarts = 0\n",
    "        self.t_epoch = -1\n",
    "        self.epoch = -1\n",
    "        \n",
    "        self.eta_min = 0\n",
    "        self.eta_max = 1\n",
    "        \n",
    "        self.end_of_period = False\n",
    "        self.batch_increments = []\n",
    "        self._set_batch_increment()\n",
    "        \n",
    "    def _on_restart(self):\n",
    "        if self.eta_on_restart_cb is not None:\n",
    "            self.eta_min, self.eta_max = self.eta_on_restart_cb(self.eta_min,\n",
    "                                                                self.eta_max)\n",
    "            \n",
    "    def _on_iteration(self):\n",
    "        if self.eta_on_iteration_cb is not None:\n",
    "            self.eta_min, self.eta_max = self.eta_on_iteration_cb(self.eta_min,\n",
    "                                                                  self.eta_max,\n",
    "                                                                  self.total_iterations)\n",
    "            \n",
    "    def get_lr(self, t_cur):\n",
    "        eta_t = (self.eta_min + (self.eta_max - self.eta_min)\n",
    "                 * self.policy_fn(t_cur, self.restart_period))\n",
    "        \n",
    "        weight_decay_norm_multi = math.sqrt(self.batch_size /\n",
    "                                            (self.epoch_size *\n",
    "                                             self.restart_period))\n",
    "        \n",
    "        lrs = [min_lr + (base_lr - min_lr) * eta_t for base_lr, min_lr\n",
    "               in zip(self.base_lrs, self.min_lrs)]\n",
    "        weight_decays = [base_weight_decay #* eta_t * weight_decay_norm_multi\n",
    "                         for base_weight_decay in self.base_weight_decays]\n",
    "        \n",
    "        if (self.t_epoch + 1) % self.restart_period < self.t_epoch:\n",
    "            self.end_of_period = True\n",
    "            \n",
    "        if self.t_epoch % self.restart_period < self.t_epoch:\n",
    "            if self.verbose:\n",
    "                print(\"Restart {} at epoch {}\".format(self.restarts + 1,\n",
    "                                                      self.last_epoch))\n",
    "            self.restart_period = math.ceil(self.restart_period * self.t_mult)\n",
    "            self.restarts += 1\n",
    "            self.t_epoch = 0\n",
    "            self._on_restart()\n",
    "            self.end_of_period = False\n",
    "            \n",
    "        return zip(lrs, weight_decays)\n",
    "        \n",
    "    def _set_batch_increment(self):\n",
    "        d, r = divmod(self.epoch_size, self.batch_size)\n",
    "        batches_in_epoch = d + 2 if r > 0 else d + 1\n",
    "        self.iteration = 0\n",
    "        self.batch_increments = torch.linspace(0, 1, batches_in_epoch).tolist()\n",
    "        \n",
    "    def step(self):\n",
    "        self.last_epoch += 1\n",
    "        self.t_epoch += 1\n",
    "        self._set_batch_increment()\n",
    "        self.batch_step()\n",
    "        \n",
    "    def batch_step(self):\n",
    "        try:\n",
    "            t_cur = self.t_epoch + self.batch_increments[self.iteration]\n",
    "            self._on_iteration()\n",
    "            self.iteration += 1\n",
    "            self.total_iterations += 1\n",
    "        except (IndexError):\n",
    "            raise StopIteration(\"Epoch size and batch size used in the \"\n",
    "                                \"training loop and while initializing \"\n",
    "                                \"scheduler should be the same.\")\n",
    "        \n",
    "        for param_group, (lr, weight_decay) in zip(self.optimizer.param_groups,\n",
    "                                                   self.get_lr(t_cur)):\n",
    "            param_group['lr'] = lr\n",
    "            param_group['weight_decay'] = weight_decay\n",
    "\n",
    "def normalized_cut_2d(edge_index, pos):\n",
    "    row, col = edge_index\n",
    "    edge_attr = torch.norm(pos[row] - pos[col], p=2, dim=1)\n",
    "    return normalized_cut(edge_index, edge_attr, num_nodes=pos.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicReductionNetwork(nn.Module):\n",
    "    # This model clusters nearest neighbour graphs\n",
    "    # in two steps.\n",
    "    # The latent space trained to group useful features at each level\n",
    "    # of aggregration.\n",
    "    # This allows single quantities to be regressed from complex point counts\n",
    "    # in a location and orientation invariant way.\n",
    "    # One encoding layer is used to abstract away the input features.\n",
    "    def __init__(self, input_dim=5, hidden_dim=64, output_dim=1, k=16, aggr='add',\n",
    "                 norm=torch.tensor([1./500., 1./500., 1./54., 1/25., 1./1000.])):\n",
    "        super(DynamicReductionNetwork, self).__init__()\n",
    "\n",
    "        self.datanorm = nn.Parameter(norm)\n",
    "\n",
    "        self.k = k\n",
    "        start_width = 2 * hidden_dim\n",
    "        middle_width = 3 * hidden_dim // 2\n",
    "\n",
    "        self.inputnet =  nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim//2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        convnn1 = nn.Sequential(nn.Linear(start_width, middle_width),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(middle_width, hidden_dim),\n",
    "                                nn.ELU(),\n",
    "                                )\n",
    "        convnn2 = nn.Sequential(nn.Linear(start_width, middle_width),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(middle_width, hidden_dim),\n",
    "                                nn.ELU(),\n",
    "                                )\n",
    "        self.edgeconv1 = EdgeConv(nn=convnn1, aggr=aggr)\n",
    "        self.edgeconv2 = EdgeConv(nn=convnn2, aggr=aggr)\n",
    "        \n",
    "        self.output = nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n",
    "                                    nn.ELU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim//2),\n",
    "                                    nn.ELU(),\n",
    "                                    nn.Linear(hidden_dim//2, output_dim))\n",
    "\n",
    "    def forward(self, data):\n",
    "        data.x = self.datanorm * data.x\n",
    "        data.x = self.inputnet(data.x)\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "        data.edge_index = to_undirected(knn_graph(data.x, self.k, data.batch, loop=False, flow=self.edgeconv1.flow))\n",
    "        data.x = self.edgeconv1(data.x, data.edge_index)\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "\n",
    "        weight = normalized_cut_2d(data.edge_index, data.x)\n",
    "        cluster = graclus(data.edge_index, weight, data.x.size(0))\n",
    "        data.edge_attr = None\n",
    "        data = max_pool(cluster, data)\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "\n",
    "        data.edge_index = to_undirected(knn_graph(data.x, self.k, data.batch, loop=False, flow=self.edgeconv2.flow))\n",
    "        data.x = self.edgeconv2(data.x, data.edge_index)\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "\n",
    "        weight = normalized_cut_2d(data.edge_index, data.x)\n",
    "        cluster = graclus(data.edge_index, weight, data.x.size(0))\n",
    "        x, batch = max_pool_x(cluster, data.x, data.batch)\n",
    "        print(\"x shape: \", x.size())\n",
    "\n",
    "        x = global_max_pool(x, batch)\n",
    "        print(\"x shape: \", x.size())\n",
    "        print(self.output(x).squeeze(-1).size())\n",
    "        return self.output(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 256\n",
      "features -> 1\n",
      "classes -> 10\n",
      "hidden_dim = 20\n",
      "Model: \n",
      "Net(\n",
      "  (drn): DynamicReductionNetwork(\n",
      "    (inputnet): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=10, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=10, out_features=20, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "      (5): ELU(alpha=1.0)\n",
      "    )\n",
      "    (edgeconv1): EdgeConv(nn=Sequential(\n",
      "      (0): Linear(in_features=40, out_features=30, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "    ))\n",
      "    (edgeconv2): EdgeConv(nn=Sequential(\n",
      "      (0): Linear(in_features=40, out_features=30, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "    ))\n",
      "    (output): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=20, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Parameters: 5123\n",
      "data x shape:  torch.Size([19200, 1])\n",
      "data x shape:  torch.Size([19200, 3])\n",
      "data x shape:  torch.Size([6400, 3])\n",
      "data pos shape:  torch.Size([6400, 2])\n",
      "data batch shape:  torch.Size([6400])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([3535, 20])\n",
      "data x shape:  torch.Size([3535, 20])\n",
      "x shape:  torch.Size([1959, 20])\n",
      "x shape:  torch.Size([256, 20])\n",
      "torch.Size([256, 10])\n",
      "result: \n",
      "tensor([[-2.3009, -2.3545, -2.4802,  ..., -2.3395, -2.1199, -2.3870],\n",
      "        [-2.2186, -2.3926, -2.4598,  ..., -2.3712, -2.1463, -2.4002],\n",
      "        [-2.2750, -2.3528, -2.4808,  ..., -2.3340, -2.1211, -2.3855],\n",
      "        ...,\n",
      "        [-2.2159, -2.3830, -2.4737,  ..., -2.3678, -2.1617, -2.3955],\n",
      "        [-2.2502, -2.3606, -2.4781,  ..., -2.3446, -2.1310, -2.3759],\n",
      "        [-2.2714, -2.3650, -2.4793,  ..., -2.3458, -2.1302, -2.3862]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([256, 10])\n",
      "pred: \n",
      "torch.return_types.max(\n",
      "values=tensor([-2.1199, -2.1463, -2.1211, -2.1370, -2.1621, -2.1449, -2.1203, -2.1134,\n",
      "        -2.1186, -2.1317, -2.1356, -2.1417, -2.1496, -2.1250, -2.1107, -2.1389,\n",
      "        -2.1419, -2.1234, -2.1300, -2.1157, -2.1192, -2.1387, -2.1437, -2.1376,\n",
      "        -2.1523, -2.1308, -2.1154, -2.1441, -2.1304, -2.1158, -2.1213, -2.1319,\n",
      "        -2.1432, -2.1146, -2.1212, -2.1459, -2.1509, -2.1408, -2.1287, -2.1176,\n",
      "        -2.1219, -2.1045, -2.1322, -2.1239, -2.1618, -2.1155, -2.1253, -2.1298,\n",
      "        -2.1410, -2.1608, -2.1362, -2.1424, -2.1403, -2.1061, -2.1266, -2.1360,\n",
      "        -2.1354, -2.1261, -2.1406, -2.1561, -2.1257, -2.1437, -2.1528, -2.1127,\n",
      "        -2.1480, -2.1334, -2.1482, -2.1681, -2.1438, -2.1393, -2.1284, -2.1551,\n",
      "        -2.1398, -2.1643, -2.1340, -2.1336, -2.1578, -2.1504, -2.1271, -2.1258,\n",
      "        -2.1481, -2.1271, -2.1239, -2.1396, -2.1404, -2.1405, -2.1249, -2.1388,\n",
      "        -2.1531, -2.1369, -2.1344, -2.1289, -2.1292, -2.1467, -2.1295, -2.1088,\n",
      "        -2.1260, -2.1363, -2.1189, -2.1540, -2.1516, -2.1786, -2.1222, -2.1306,\n",
      "        -2.1275, -2.1470, -2.1199, -2.1103, -2.1494, -2.1335, -2.1541, -2.1265,\n",
      "        -2.1347, -2.1449, -2.1441, -2.1467, -2.1552, -2.1497, -2.1394, -2.1258,\n",
      "        -2.1249, -2.1194, -2.1579, -2.1618, -2.1225, -2.1348, -2.1535, -2.1148,\n",
      "        -2.1366, -2.1339, -2.1397, -2.1437, -2.1369, -2.1359, -2.1276, -2.1261,\n",
      "        -2.1365, -2.1240, -2.1428, -2.1466, -2.1739, -2.1655, -2.1218, -2.1259,\n",
      "        -2.1247, -2.1273, -2.1181, -2.1597, -2.1203, -2.1348, -2.1234, -2.1435,\n",
      "        -2.1317, -2.1488, -2.1491, -2.1651, -2.1302, -2.1335, -2.1223, -2.1248,\n",
      "        -2.1194, -2.1903, -2.1495, -2.1217, -2.1273, -2.1396, -2.1162, -2.1371,\n",
      "        -2.1128, -2.1366, -2.1196, -2.1370, -2.1646, -2.1444, -2.1343, -2.1407,\n",
      "        -2.1270, -2.1416, -2.1423, -2.1336, -2.1495, -2.1372, -2.1801, -2.1339,\n",
      "        -2.1353, -2.1053, -2.1409, -2.1064, -2.1597, -2.1258, -2.1367, -2.1060,\n",
      "        -2.1352, -2.1483, -2.1412, -2.1355, -2.1497, -2.1293, -2.1669, -2.1373,\n",
      "        -2.1203, -2.1520, -2.1271, -2.1283, -2.1419, -2.1255, -2.1593, -2.1178,\n",
      "        -2.1433, -2.1363, -2.1438, -2.1678, -2.1512, -2.1322, -2.1036, -2.1223,\n",
      "        -2.1440, -2.1679, -2.1387, -2.1639, -2.1321, -2.1495, -2.1358, -2.1483,\n",
      "        -2.1499, -2.1510, -2.1303, -2.1287, -2.1271, -2.1471, -2.1350, -2.1420,\n",
      "        -2.1488, -2.1528, -2.1308, -2.1455, -2.1682, -2.1141, -2.1327, -2.1150,\n",
      "        -2.1536, -2.1524, -2.1238, -2.1206, -2.1530, -2.1253, -2.1405, -2.1433,\n",
      "        -2.1590, -2.1340, -2.1404, -2.1113, -2.1216, -2.1617, -2.1310, -2.1302],\n",
      "       grad_fn=<MaxBackward0>),\n",
      "indices=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]))\n",
      "truth: \n",
      "tensor([5, 5, 1, 1, 4, 6, 2, 2, 8, 4, 1, 1, 5, 1, 6, 1, 2, 6, 6, 8, 6, 7, 4, 2,\n",
      "        4, 2, 7, 4, 2, 1, 0, 1, 7, 8, 0, 8, 8, 1, 3, 8, 8, 1, 6, 2, 4, 1, 2, 5,\n",
      "        8, 0, 6, 2, 0, 6, 8, 5, 4, 5, 9, 9, 6, 8, 2, 1, 3, 5, 8, 4, 3, 0, 7, 4,\n",
      "        4, 9, 3, 8, 0, 6, 7, 3, 9, 8, 3, 4, 3, 1, 2, 7, 0, 6, 6, 5, 1, 4, 9, 7,\n",
      "        4, 9, 6, 5, 9, 9, 3, 2, 5, 8, 6, 4, 6, 4, 6, 0, 0, 7, 5, 6, 5, 9, 1, 4,\n",
      "        5, 3, 2, 2, 8, 6, 9, 7, 1, 5, 7, 3, 2, 3, 2, 2, 3, 6, 8, 2, 1, 3, 9, 7,\n",
      "        0, 4, 4, 6, 9, 0, 3, 8, 0, 1, 2, 4, 0, 9, 7, 8, 7, 3, 9, 7, 6, 6, 1, 5,\n",
      "        0, 7, 3, 8, 6, 7, 3, 4, 3, 7, 4, 9, 7, 4, 6, 0, 8, 0, 1, 3, 7, 8, 2, 8,\n",
      "        7, 6, 9, 0, 3, 3, 3, 8, 8, 4, 3, 1, 1, 5, 5, 5, 7, 6, 9, 2, 5, 1, 5, 0,\n",
      "        8, 6, 1, 8, 0, 0, 9, 2, 7, 3, 6, 7, 1, 3, 7, 7, 6, 9, 9, 3, 5, 3, 5, 4,\n",
      "        2, 2, 4, 4, 5, 9, 8, 1, 2, 7, 6, 0, 3, 1, 4, 1])\n",
      "loss tensor(2.3034, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([19200, 1])\n",
      "data x shape:  torch.Size([19200, 3])\n",
      "data x shape:  torch.Size([6400, 3])\n",
      "data pos shape:  torch.Size([6400, 2])\n",
      "data batch shape:  torch.Size([6400])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([3539, 20])\n",
      "data x shape:  torch.Size([3539, 20])\n",
      "x shape:  torch.Size([1959, 20])\n",
      "x shape:  torch.Size([256, 20])\n",
      "torch.Size([256, 10])\n",
      "result: \n",
      "tensor([[-2.3127, -2.3232, -2.4529,  ..., -2.3204, -2.1450, -2.3984],\n",
      "        [-2.2709, -2.3329, -2.4531,  ..., -2.3348, -2.1505, -2.3923],\n",
      "        [-2.3180, -2.3421, -2.4572,  ..., -2.3240, -2.1447, -2.4016],\n",
      "        ...,\n",
      "        [-2.2709, -2.3478, -2.4636,  ..., -2.3335, -2.1479, -2.3889],\n",
      "        [-2.3496, -2.3141, -2.4541,  ..., -2.3097, -2.1293, -2.4019],\n",
      "        [-2.2425, -2.3532, -2.4552,  ..., -2.3499, -2.1932, -2.4064]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([256, 10])\n",
      "pred: \n",
      "torch.return_types.max(\n",
      "values=tensor([-2.1450, -2.1505, -2.1447, -2.1296, -2.1461, -2.1487, -2.1404, -2.1654,\n",
      "        -2.1765, -2.1602, -2.1398, -2.1347, -2.1842, -2.1768, -2.1353, -2.1714,\n",
      "        -2.1612, -2.1754, -2.1671, -2.1392, -2.1306, -2.1531, -2.1442, -2.1541,\n",
      "        -2.1543, -2.1519, -2.1557, -2.1518, -2.1616, -2.1589, -2.1547, -2.1269,\n",
      "        -2.1493, -2.1898, -2.1384, -2.1277, -2.1627, -2.1353, -2.1362, -2.1668,\n",
      "        -2.1556, -2.1718, -2.1383, -2.1611, -2.1374, -2.1510, -2.1705, -2.1623,\n",
      "        -2.1320, -2.1768, -2.1543, -2.1516, -2.1409, -2.1269, -2.1456, -2.1494,\n",
      "        -2.1499, -2.1361, -2.1312, -2.1513, -2.1382, -2.1441, -2.1429, -2.1416,\n",
      "        -2.1620, -2.1440, -2.1380, -2.1524, -2.1590, -2.1611, -2.1518, -2.1522,\n",
      "        -2.1395, -2.1469, -2.1387, -2.1517, -2.1548, -2.1436, -2.1500, -2.1439,\n",
      "        -2.1553, -2.1514, -2.1514, -2.1755, -2.1591, -2.1346, -2.1547, -2.1305,\n",
      "        -2.1634, -2.1605, -2.1530, -2.1462, -2.1626, -2.1406, -2.1457, -2.1553,\n",
      "        -2.1516, -2.1414, -2.1836, -2.1516, -2.1455, -2.1497, -2.1438, -2.1466,\n",
      "        -2.1443, -2.1191, -2.1464, -2.1472, -2.1278, -2.1475, -2.1332, -2.1457,\n",
      "        -2.1416, -2.1598, -2.1370, -2.1747, -2.1533, -2.1258, -2.1431, -2.1588,\n",
      "        -2.1527, -2.1902, -2.1659, -2.1758, -2.1244, -2.1331, -2.1401, -2.1637,\n",
      "        -2.1647, -2.1540, -2.1428, -2.1586, -2.1515, -2.1732, -2.1379, -2.1404,\n",
      "        -2.1633, -2.1629, -2.1416, -2.1442, -2.1709, -2.1339, -2.1419, -2.1572,\n",
      "        -2.1283, -2.1492, -2.1487, -2.1339, -2.1417, -2.1574, -2.1378, -2.1735,\n",
      "        -2.1593, -2.1750, -2.1686, -2.1464, -2.1628, -2.1512, -2.1560, -2.1571,\n",
      "        -2.1366, -2.1297, -2.1326, -2.1322, -2.1900, -2.1385, -2.1329, -2.1460,\n",
      "        -2.1410, -2.1478, -2.1536, -2.1527, -2.1626, -2.1531, -2.1457, -2.1394,\n",
      "        -2.1527, -2.1608, -2.1536, -2.1394, -2.1325, -2.1439, -2.1399, -2.1547,\n",
      "        -2.1652, -2.1408, -2.1459, -2.1514, -2.1459, -2.1475, -2.1454, -2.1450,\n",
      "        -2.1426, -2.1456, -2.1858, -2.1727, -2.1374, -2.1459, -2.1405, -2.1427,\n",
      "        -2.1374, -2.1373, -2.1417, -2.1479, -2.1310, -2.1776, -2.1748, -2.1557,\n",
      "        -2.1456, -2.1421, -2.1564, -2.1468, -2.1559, -2.1468, -2.1400, -2.1329,\n",
      "        -2.1392, -2.1445, -2.1594, -2.1549, -2.1467, -2.1590, -2.1388, -2.1447,\n",
      "        -2.1549, -2.1443, -2.1570, -2.1396, -2.1679, -2.1713, -2.1551, -2.1640,\n",
      "        -2.1805, -2.1638, -2.1367, -2.1408, -2.1770, -2.1580, -2.1497, -2.1492,\n",
      "        -2.1401, -2.1249, -2.1541, -2.1360, -2.1535, -2.1797, -2.1571, -2.1341,\n",
      "        -2.1385, -2.1450, -2.1448, -2.1244, -2.1446, -2.1479, -2.1293, -2.1932],\n",
      "       grad_fn=<MaxBackward0>),\n",
      "indices=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]))\n",
      "truth: \n",
      "tensor([6, 2, 0, 3, 1, 0, 4, 8, 1, 9, 0, 1, 7, 8, 7, 7, 6, 6, 4, 3, 0, 5, 3, 4,\n",
      "        7, 9, 7, 2, 5, 2, 7, 1, 7, 2, 7, 7, 3, 0, 5, 1, 3, 6, 9, 7, 0, 1, 3, 7,\n",
      "        8, 4, 1, 5, 9, 6, 7, 8, 6, 3, 0, 3, 6, 6, 7, 5, 7, 9, 4, 4, 8, 1, 5, 1,\n",
      "        0, 3, 4, 7, 6, 8, 6, 9, 9, 1, 1, 9, 2, 3, 6, 8, 0, 4, 5, 5, 3, 7, 7, 1,\n",
      "        8, 8, 2, 1, 8, 1, 8, 0, 4, 0, 3, 1, 3, 2, 9, 0, 0, 3, 5, 3, 5, 9, 0, 6,\n",
      "        0, 6, 2, 4, 5, 5, 0, 6, 5, 2, 5, 4, 6, 6, 4, 9, 2, 7, 0, 1, 8, 7, 2, 8,\n",
      "        0, 8, 1, 7, 9, 6, 1, 7, 7, 3, 2, 4, 0, 9, 5, 8, 1, 4, 7, 7, 2, 8, 0, 7,\n",
      "        0, 8, 6, 3, 4, 2, 9, 9, 1, 6, 1, 0, 0, 8, 9, 2, 3, 0, 7, 7, 2, 0, 8, 5,\n",
      "        2, 7, 2, 2, 1, 2, 4, 7, 0, 0, 2, 8, 2, 5, 7, 5, 8, 2, 4, 0, 2, 3, 3, 2,\n",
      "        8, 0, 7, 7, 4, 2, 3, 4, 4, 9, 2, 4, 3, 0, 1, 3, 5, 2, 0, 7, 0, 9, 7, 4,\n",
      "        8, 1, 7, 9, 5, 9, 8, 7, 7, 8, 8, 0, 3, 8, 9, 4])\n",
      "loss tensor(2.3070, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data x shape:  torch.Size([19200, 1])\n",
      "data x shape:  torch.Size([19200, 3])\n",
      "data x shape:  torch.Size([6400, 3])\n",
      "data pos shape:  torch.Size([6400, 2])\n",
      "data batch shape:  torch.Size([6400])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([3536, 20])\n",
      "data x shape:  torch.Size([3536, 20])\n",
      "x shape:  torch.Size([1930, 20])\n",
      "x shape:  torch.Size([256, 20])\n",
      "torch.Size([256, 10])\n",
      "result: \n",
      "tensor([[-2.2075, -2.3378, -2.4285,  ..., -2.3400, -2.1987, -2.4079],\n",
      "        [-2.2661, -2.3095, -2.4310,  ..., -2.3198, -2.1775, -2.4053],\n",
      "        [-2.2047, -2.3516, -2.4160,  ..., -2.3535, -2.2214, -2.4203],\n",
      "        ...,\n",
      "        [-2.2755, -2.3098, -2.4207,  ..., -2.3143, -2.1697, -2.4092],\n",
      "        [-2.2743, -2.3057, -2.4298,  ..., -2.3076, -2.1598, -2.4007],\n",
      "        [-2.2492, -2.3119, -2.4324,  ..., -2.3274, -2.1932, -2.4090]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([256, 10])\n",
      "pred: \n",
      "torch.return_types.max(\n",
      "values=tensor([-2.1987, -2.1775, -2.2027, -2.1849, -2.1800, -2.1921, -2.1773, -2.2084,\n",
      "        -2.1634, -2.1917, -2.1570, -2.1643, -2.2039, -2.1725, -2.1557, -2.1787,\n",
      "        -2.1986, -2.1746, -2.1926, -2.1959, -2.1940, -2.1790, -2.1827, -2.1821,\n",
      "        -2.2037, -2.1813, -2.1753, -2.1731, -2.1767, -2.1679, -2.1749, -2.1934,\n",
      "        -2.1747, -2.1747, -2.1922, -2.1913, -2.1693, -2.1989, -2.1779, -2.1937,\n",
      "        -2.1621, -2.1811, -2.1579, -2.1591, -2.2005, -2.1601, -2.1931, -2.1823,\n",
      "        -2.1943, -2.1984, -2.1783, -2.1828, -2.1843, -2.1956, -2.2101, -2.1598,\n",
      "        -2.1651, -2.1841, -2.1837, -2.1892, -2.1921, -2.1733, -2.1728, -2.1864,\n",
      "        -2.1733, -2.2103, -2.1730, -2.1895, -2.1996, -2.1792, -2.2048, -2.1945,\n",
      "        -2.1844, -2.1806, -2.1881, -2.1774, -2.1832, -2.2090, -2.1707, -2.1823,\n",
      "        -2.1739, -2.1715, -2.1886, -2.1842, -2.1986, -2.2127, -2.1852, -2.1604,\n",
      "        -2.2127, -2.1846, -2.1769, -2.1712, -2.1805, -2.1749, -2.1814, -2.1654,\n",
      "        -2.1802, -2.1679, -2.1754, -2.1762, -2.1958, -2.1630, -2.1742, -2.1696,\n",
      "        -2.1773, -2.1757, -2.1985, -2.1861, -2.1892, -2.1904, -2.1843, -2.1965,\n",
      "        -2.1820, -2.1879, -2.1486, -2.1913, -2.1737, -2.1836, -2.1858, -2.1818,\n",
      "        -2.1889, -2.1827, -2.1626, -2.1771, -2.1725, -2.1829, -2.1853, -2.1892,\n",
      "        -2.1791, -2.1725, -2.1679, -2.1873, -2.1601, -2.1877, -2.1845, -2.1721,\n",
      "        -2.1872, -2.1699, -2.1853, -2.1885, -2.1689, -2.1867, -2.1616, -2.1639,\n",
      "        -2.1836, -2.2087, -2.1994, -2.1700, -2.1693, -2.1742, -2.1878, -2.1557,\n",
      "        -2.1876, -2.2072, -2.1727, -2.1890, -2.1632, -2.1684, -2.1753, -2.1913,\n",
      "        -2.1666, -2.1578, -2.1748, -2.1954, -2.1610, -2.1838, -2.2044, -2.1681,\n",
      "        -2.1830, -2.1859, -2.2057, -2.1858, -2.1638, -2.1623, -2.1819, -2.1698,\n",
      "        -2.1760, -2.1786, -2.2058, -2.2020, -2.2033, -2.2025, -2.1911, -2.1662,\n",
      "        -2.1875, -2.2140, -2.1807, -2.1641, -2.1539, -2.1709, -2.1934, -2.2041,\n",
      "        -2.1710, -2.1730, -2.1634, -2.1901, -2.1733, -2.2098, -2.1991, -2.1762,\n",
      "        -2.2172, -2.1698, -2.1759, -2.1691, -2.1852, -2.1884, -2.1715, -2.1674,\n",
      "        -2.1770, -2.1872, -2.1690, -2.1880, -2.1929, -2.1803, -2.1781, -2.1674,\n",
      "        -2.1937, -2.1795, -2.1605, -2.2222, -2.1746, -2.1796, -2.1840, -2.1663,\n",
      "        -2.1928, -2.1699, -2.1770, -2.1999, -2.1731, -2.1799, -2.2031, -2.1819,\n",
      "        -2.1861, -2.1809, -2.1707, -2.1824, -2.1607, -2.1896, -2.1915, -2.1959,\n",
      "        -2.1837, -2.1854, -2.1882, -2.1836, -2.1869, -2.1840, -2.1808, -2.1782,\n",
      "        -2.1856, -2.1721, -2.1963, -2.1818, -2.1818, -2.1697, -2.1598, -2.1932],\n",
      "       grad_fn=<MaxBackward0>),\n",
      "indices=tensor([8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 4, 8, 8, 8, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 4, 8, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]))\n",
      "truth: \n",
      "tensor([5, 8, 5, 1, 7, 4, 8, 9, 7, 8, 2, 7, 2, 3, 5, 4, 2, 8, 8, 3, 6, 9, 3, 9,\n",
      "        2, 6, 5, 4, 5, 4, 4, 8, 8, 8, 9, 9, 2, 8, 1, 0, 0, 2, 5, 4, 8, 7, 4, 8,\n",
      "        0, 1, 3, 9, 1, 0, 9, 3, 1, 9, 7, 8, 9, 4, 7, 8, 9, 6, 2, 1, 0, 4, 6, 3,\n",
      "        9, 4, 7, 4, 0, 6, 8, 8, 7, 2, 6, 1, 2, 5, 7, 9, 1, 3, 7, 3, 6, 8, 3, 5,\n",
      "        8, 8, 4, 3, 3, 9, 2, 7, 6, 8, 5, 5, 1, 3, 6, 0, 9, 5, 8, 3, 3, 9, 4, 9,\n",
      "        7, 1, 0, 3, 6, 3, 4, 2, 8, 4, 6, 7, 9, 5, 5, 3, 8, 9, 8, 9, 1, 7, 7, 5,\n",
      "        7, 1, 6, 7, 1, 8, 3, 3, 4, 7, 4, 3, 7, 1, 6, 6, 8, 9, 0, 0, 7, 2, 2, 3,\n",
      "        6, 4, 6, 1, 9, 8, 3, 0, 8, 7, 1, 2, 0, 7, 5, 7, 8, 8, 7, 7, 8, 0, 0, 7,\n",
      "        7, 0, 5, 5, 9, 4, 5, 2, 2, 9, 5, 8, 6, 6, 5, 6, 4, 5, 6, 6, 4, 7, 2, 2,\n",
      "        9, 3, 6, 0, 0, 5, 5, 9, 8, 8, 6, 6, 5, 1, 9, 0, 2, 1, 0, 8, 6, 8, 3, 0,\n",
      "        0, 3, 3, 8, 2, 9, 3, 8, 8, 5, 4, 8, 9, 9, 8, 7])\n",
      "loss tensor(2.2968, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([19200, 1])\n",
      "data x shape:  torch.Size([19200, 3])\n",
      "data x shape:  torch.Size([6400, 3])\n",
      "data pos shape:  torch.Size([6400, 2])\n",
      "data batch shape:  torch.Size([6400])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([3527, 20])\n",
      "data x shape:  torch.Size([3527, 20])\n",
      "x shape:  torch.Size([1948, 20])\n",
      "x shape:  torch.Size([256, 20])\n",
      "torch.Size([256, 10])\n",
      "result: \n",
      "tensor([[-2.2758, -2.3064, -2.4420,  ..., -2.2878, -2.1497, -2.3921],\n",
      "        [-2.3000, -2.3207, -2.4364,  ..., -2.2845, -2.1424, -2.3996],\n",
      "        [-2.2929, -2.3084, -2.4368,  ..., -2.2826, -2.1274, -2.3916],\n",
      "        ...,\n",
      "        [-2.2989, -2.3046, -2.4332,  ..., -2.2846, -2.1373, -2.3947],\n",
      "        [-2.3306, -2.3063, -2.4358,  ..., -2.2809, -2.1282, -2.3916],\n",
      "        [-2.3045, -2.3121, -2.4354,  ..., -2.2822, -2.1279, -2.3944]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([256, 10])\n",
      "pred: \n",
      "torch.return_types.max(\n",
      "values=tensor([-2.1497, -2.1424, -2.1274, -2.1255, -2.1225, -2.1460, -2.1380, -2.1423,\n",
      "        -2.1518, -2.1517, -2.1344, -2.1328, -2.1319, -2.1296, -2.1092, -2.1646,\n",
      "        -2.1433, -2.1358, -2.1146, -2.1353, -2.1186, -2.1249, -2.1294, -2.1207,\n",
      "        -2.1162, -2.1338, -2.1372, -2.1304, -2.1287, -2.1145, -2.1285, -2.1361,\n",
      "        -2.1628, -2.1280, -2.1398, -2.1262, -2.1401, -2.1184, -2.1630, -2.1343,\n",
      "        -2.1164, -2.1404, -2.1331, -2.1281, -2.1570, -2.1409, -2.1350, -2.1353,\n",
      "        -2.1342, -2.1358, -2.1466, -2.1309, -2.1217, -2.1333, -2.1159, -2.1409,\n",
      "        -2.1480, -2.1453, -2.1528, -2.1293, -2.1368, -2.1313, -2.1179, -2.1552,\n",
      "        -2.1223, -2.1505, -2.1602, -2.1362, -2.1273, -2.1164, -2.1628, -2.1306,\n",
      "        -2.1386, -2.1452, -2.1316, -2.1213, -2.1235, -2.1443, -2.1637, -2.1312,\n",
      "        -2.1277, -2.1332, -2.1195, -2.1470, -2.1198, -2.1375, -2.1638, -2.1209,\n",
      "        -2.1258, -2.1457, -2.1268, -2.1367, -2.1737, -2.1286, -2.1247, -2.1363,\n",
      "        -2.1346, -2.1428, -2.1230, -2.1241, -2.1342, -2.1589, -2.1253, -2.1254,\n",
      "        -2.1396, -2.1235, -2.1195, -2.1454, -2.1137, -2.1436, -2.1411, -2.1316,\n",
      "        -2.1414, -2.1241, -2.1521, -2.1433, -2.1337, -2.1509, -2.1218, -2.1312,\n",
      "        -2.1480, -2.1453, -2.0955, -2.1334, -2.1307, -2.1544, -2.1365, -2.1480,\n",
      "        -2.1357, -2.1383, -2.1352, -2.1410, -2.1129, -2.1469, -2.1263, -2.1558,\n",
      "        -2.1438, -2.1440, -2.1491, -2.1560, -2.1358, -2.1156, -2.1273, -2.1309,\n",
      "        -2.1428, -2.1382, -2.1391, -2.1512, -2.1548, -2.1241, -2.1793, -2.1182,\n",
      "        -2.1274, -2.1466, -2.1238, -2.1483, -2.1386, -2.1430, -2.1295, -2.1641,\n",
      "        -2.1375, -2.1346, -2.1398, -2.1442, -2.1464, -2.1444, -2.1406, -2.1571,\n",
      "        -2.1310, -2.1343, -2.1236, -2.1667, -2.1413, -2.1396, -2.1515, -2.1351,\n",
      "        -2.1457, -2.1365, -2.1270, -2.1394, -2.1508, -2.1400, -2.1390, -2.1444,\n",
      "        -2.1367, -2.1459, -2.1319, -2.1436, -2.1322, -2.1330, -2.1215, -2.1391,\n",
      "        -2.1324, -2.1199, -2.1214, -2.1221, -2.1437, -2.1388, -2.1231, -2.1347,\n",
      "        -2.1246, -2.1349, -2.1504, -2.1518, -2.1418, -2.1435, -2.1323, -2.1474,\n",
      "        -2.1333, -2.1311, -2.1216, -2.1668, -2.1592, -2.1192, -2.1335, -2.1363,\n",
      "        -2.1676, -2.1459, -2.1173, -2.1279, -2.1253, -2.1234, -2.1401, -2.1506,\n",
      "        -2.1505, -2.1403, -2.1480, -2.1517, -2.1153, -2.1400, -2.1223, -2.1388,\n",
      "        -2.1385, -2.1276, -2.1357, -2.1353, -2.1481, -2.1198, -2.1379, -2.1194,\n",
      "        -2.1178, -2.1546, -2.1588, -2.1304, -2.1517, -2.1233, -2.1312, -2.1259,\n",
      "        -2.1243, -2.1633, -2.1473, -2.1247, -2.1386, -2.1373, -2.1282, -2.1279],\n",
      "       grad_fn=<MaxBackward0>),\n",
      "indices=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]))\n",
      "truth: \n",
      "tensor([5, 6, 0, 4, 5, 6, 0, 9, 0, 3, 2, 7, 0, 9, 8, 3, 1, 3, 7, 4, 9, 3, 6, 7,\n",
      "        0, 7, 8, 9, 9, 1, 8, 3, 4, 7, 1, 1, 1, 8, 2, 4, 3, 0, 3, 4, 0, 9, 9, 6,\n",
      "        3, 5, 8, 9, 3, 4, 0, 1, 3, 7, 6, 3, 2, 6, 3, 2, 0, 4, 0, 4, 7, 4, 5, 1,\n",
      "        8, 7, 4, 7, 4, 8, 4, 3, 0, 7, 5, 3, 8, 5, 5, 8, 4, 8, 0, 2, 2, 4, 8, 1,\n",
      "        7, 8, 9, 0, 5, 8, 5, 7, 1, 1, 7, 2, 6, 1, 6, 0, 7, 2, 0, 9, 2, 9, 7, 5,\n",
      "        6, 4, 7, 7, 4, 3, 8, 0, 7, 8, 0, 9, 4, 4, 0, 4, 5, 1, 0, 2, 0, 3, 3, 6,\n",
      "        0, 1, 1, 4, 6, 9, 0, 2, 2, 5, 1, 6, 3, 2, 3, 4, 5, 7, 8, 6, 9, 7, 2, 5,\n",
      "        7, 9, 5, 7, 6, 7, 3, 9, 0, 4, 0, 4, 7, 7, 2, 3, 0, 4, 4, 4, 1, 8, 6, 2,\n",
      "        4, 8, 9, 5, 4, 4, 8, 6, 8, 6, 4, 2, 5, 5, 2, 6, 1, 2, 1, 3, 6, 3, 3, 2,\n",
      "        4, 1, 2, 3, 4, 9, 1, 4, 3, 5, 2, 6, 8, 0, 4, 8, 1, 4, 2, 0, 0, 7, 9, 0,\n",
      "        1, 9, 5, 2, 4, 7, 6, 8, 8, 6, 4, 0, 0, 3, 4, 5])\n",
      "loss tensor(2.3012, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data x shape:  torch.Size([19200, 1])\n",
      "data x shape:  torch.Size([19200, 3])\n",
      "data x shape:  torch.Size([6400, 3])\n",
      "data pos shape:  torch.Size([6400, 2])\n",
      "data batch shape:  torch.Size([6400])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([3515, 20])\n",
      "data x shape:  torch.Size([3515, 20])\n",
      "x shape:  torch.Size([1931, 20])\n",
      "x shape:  torch.Size([256, 20])\n",
      "torch.Size([256, 10])\n",
      "result: \n",
      "tensor([[-2.2999, -2.3259, -2.4295,  ..., -2.2770, -2.1470, -2.4029],\n",
      "        [-2.2932, -2.3196, -2.4385,  ..., -2.2747, -2.1336, -2.3900],\n",
      "        [-2.3262, -2.3110, -2.4240,  ..., -2.2700, -2.1194, -2.3924],\n",
      "        ...,\n",
      "        [-2.2876, -2.3341, -2.4272,  ..., -2.2852, -2.1372, -2.3952],\n",
      "        [-2.3164, -2.3148, -2.4287,  ..., -2.2682, -2.1482, -2.4068],\n",
      "        [-2.3376, -2.3198, -2.4384,  ..., -2.2544, -2.1166, -2.3932]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([256, 10])\n",
      "pred: \n",
      "torch.return_types.max(\n",
      "values=tensor([-2.1470, -2.1336, -2.1194, -2.1427, -2.1581, -2.1370, -2.1122, -2.1247,\n",
      "        -2.1476, -2.1417, -2.1393, -2.1211, -2.1135, -2.1399, -2.1342, -2.1380,\n",
      "        -2.1266, -2.1682, -2.1312, -2.1396, -2.1545, -2.1485, -2.1374, -2.1686,\n",
      "        -2.1386, -2.1279, -2.1236, -2.1280, -2.1397, -2.1232, -2.1355, -2.1389,\n",
      "        -2.1475, -2.1309, -2.1379, -2.1439, -2.1458, -2.1431, -2.1451, -2.1203,\n",
      "        -2.1258, -2.1403, -2.1376, -2.1201, -2.1197, -2.1385, -2.1223, -2.1366,\n",
      "        -2.1576, -2.1475, -2.1667, -2.1201, -2.1294, -2.1357, -2.1407, -2.1244,\n",
      "        -2.1302, -2.1332, -2.1445, -2.1428, -2.1307, -2.1263, -2.1388, -2.1283,\n",
      "        -2.1407, -2.1418, -2.1426, -2.1381, -2.1614, -2.1433, -2.1423, -2.1574,\n",
      "        -2.1318, -2.1267, -2.1297, -2.1409, -2.1216, -2.1321, -2.1441, -2.1406,\n",
      "        -2.1438, -2.1297, -2.1514, -2.1463, -2.1252, -2.1264, -2.1355, -2.1386,\n",
      "        -2.1387, -2.1821, -2.1509, -2.1180, -2.1290, -2.1512, -2.1471, -2.1375,\n",
      "        -2.1138, -2.1561, -2.1485, -2.1377, -2.1480, -2.1255, -2.1398, -2.1481,\n",
      "        -2.1437, -2.1301, -2.1841, -2.1310, -2.1597, -2.1517, -2.1359, -2.1488,\n",
      "        -2.1322, -2.1200, -2.1346, -2.1550, -2.1265, -2.1206, -2.1659, -2.1333,\n",
      "        -2.1408, -2.1269, -2.1441, -2.1426, -2.1204, -2.1274, -2.1387, -2.1417,\n",
      "        -2.1346, -2.1219, -2.1535, -2.1199, -2.1246, -2.1351, -2.1421, -2.1144,\n",
      "        -2.1339, -2.1326, -2.1361, -2.1375, -2.1293, -2.1468, -2.1115, -2.1313,\n",
      "        -2.1165, -2.1356, -2.1310, -2.1340, -2.1576, -2.1351, -2.1539, -2.1337,\n",
      "        -2.1117, -2.1301, -2.1398, -2.1368, -2.1320, -2.1134, -2.1390, -2.1356,\n",
      "        -2.1572, -2.1437, -2.1301, -2.1246, -2.1335, -2.1579, -2.1278, -2.1411,\n",
      "        -2.1232, -2.1630, -2.1353, -2.1387, -2.1280, -2.1123, -2.1476, -2.1481,\n",
      "        -2.1219, -2.1287, -2.1424, -2.1196, -2.1306, -2.1272, -2.1136, -2.1300,\n",
      "        -2.1410, -2.1607, -2.1427, -2.1444, -2.1548, -2.1429, -2.1214, -2.1363,\n",
      "        -2.1269, -2.1278, -2.1449, -2.1201, -2.1361, -2.1756, -2.1580, -2.1369,\n",
      "        -2.1378, -2.1212, -2.1266, -2.1224, -2.1478, -2.1290, -2.1434, -2.1486,\n",
      "        -2.1413, -2.1357, -2.1374, -2.1361, -2.1272, -2.1313, -2.1214, -2.1453,\n",
      "        -2.1473, -2.1583, -2.1354, -2.1293, -2.1312, -2.1229, -2.1082, -2.1141,\n",
      "        -2.1395, -2.1552, -2.1401, -2.1498, -2.1298, -2.1444, -2.1280, -2.1278,\n",
      "        -2.1243, -2.1363, -2.1302, -2.1456, -2.1363, -2.1475, -2.1562, -2.1247,\n",
      "        -2.1176, -2.1444, -2.1203, -2.1384, -2.1349, -2.1315, -2.1347, -2.1405,\n",
      "        -2.1584, -2.1641, -2.1214, -2.1421, -2.1317, -2.1372, -2.1482, -2.1166],\n",
      "       grad_fn=<MaxBackward0>),\n",
      "indices=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]))\n",
      "truth: \n",
      "tensor([6, 8, 1, 4, 6, 0, 5, 3, 4, 0, 4, 0, 1, 2, 9, 4, 8, 4, 7, 8, 6, 7, 0, 7,\n",
      "        3, 3, 4, 2, 1, 0, 6, 1, 1, 0, 1, 5, 2, 6, 2, 8, 0, 3, 8, 5, 0, 7, 3, 2,\n",
      "        0, 6, 3, 3, 4, 1, 3, 6, 7, 3, 6, 6, 8, 3, 5, 8, 4, 2, 6, 0, 1, 0, 7, 4,\n",
      "        7, 9, 4, 2, 0, 9, 0, 7, 5, 1, 2, 3, 4, 5, 7, 9, 4, 7, 9, 8, 8, 0, 4, 3,\n",
      "        4, 2, 1, 1, 3, 6, 8, 4, 9, 3, 8, 5, 3, 4, 9, 1, 1, 5, 4, 6, 1, 5, 5, 7,\n",
      "        4, 1, 2, 9, 0, 0, 2, 0, 4, 1, 3, 6, 1, 5, 4, 8, 8, 0, 7, 9, 2, 3, 7, 3,\n",
      "        1, 9, 7, 9, 6, 2, 1, 9, 1, 1, 9, 8, 4, 3, 1, 6, 6, 3, 8, 9, 4, 6, 2, 0,\n",
      "        2, 4, 2, 9, 0, 9, 0, 7, 0, 5, 1, 6, 4, 1, 7, 2, 2, 6, 6, 8, 8, 4, 1, 1,\n",
      "        7, 8, 6, 5, 6, 3, 8, 6, 7, 8, 1, 4, 3, 8, 3, 5, 3, 8, 0, 9, 4, 9, 5, 8,\n",
      "        3, 2, 8, 0, 7, 6, 0, 1, 0, 6, 7, 9, 5, 6, 6, 4, 0, 1, 9, 1, 1, 1, 2, 0,\n",
      "        6, 1, 4, 0, 2, 0, 2, 1, 1, 4, 2, 7, 3, 4, 2, 6])\n",
      "loss tensor(2.3008, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([19200, 1])\n",
      "data x shape:  torch.Size([19200, 3])\n",
      "data x shape:  torch.Size([6400, 3])\n",
      "data pos shape:  torch.Size([6400, 2])\n",
      "data batch shape:  torch.Size([6400])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([3527, 20])\n",
      "data x shape:  torch.Size([3527, 20])\n",
      "x shape:  torch.Size([1938, 20])\n",
      "x shape:  torch.Size([256, 20])\n",
      "torch.Size([256, 10])\n",
      "result: \n",
      "tensor([[-2.2833, -2.3188, -2.4234,  ..., -2.2680, -2.1617, -2.4047],\n",
      "        [-2.2737, -2.3351, -2.4162,  ..., -2.2846, -2.1710, -2.4100],\n",
      "        [-2.2909, -2.3263, -2.4268,  ..., -2.2693, -2.1385, -2.3981],\n",
      "        ...,\n",
      "        [-2.2713, -2.3255, -2.4205,  ..., -2.2814, -2.1521, -2.3966],\n",
      "        [-2.2961, -2.3229, -2.4085,  ..., -2.2711, -2.1600, -2.4120],\n",
      "        [-2.3146, -2.3207, -2.4148,  ..., -2.2612, -2.1492, -2.4075]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([256, 10])\n",
      "pred: \n",
      "torch.return_types.max(\n",
      "values=tensor([-2.1617, -2.1710, -2.1385, -2.1555, -2.1606, -2.1796, -2.1521, -2.1542,\n",
      "        -2.1349, -2.1562, -2.1662, -2.1516, -2.1685, -2.1507, -2.1487, -2.1510,\n",
      "        -2.1616, -2.1398, -2.1381, -2.1654, -2.1677, -2.1651, -2.1394, -2.1564,\n",
      "        -2.1356, -2.1360, -2.1633, -2.1687, -2.1503, -2.1644, -2.1664, -2.1588,\n",
      "        -2.1450, -2.1576, -2.1449, -2.1609, -2.1594, -2.1485, -2.1413, -2.1643,\n",
      "        -2.1706, -2.1785, -2.1596, -2.1324, -2.1650, -2.1542, -2.1557, -2.1631,\n",
      "        -2.1466, -2.1684, -2.1502, -2.1525, -2.1588, -2.1485, -2.1879, -2.1450,\n",
      "        -2.1445, -2.1594, -2.1690, -2.1515, -2.1496, -2.1598, -2.1382, -2.1646,\n",
      "        -2.1696, -2.1567, -2.1658, -2.1622, -2.1546, -2.1445, -2.1381, -2.1934,\n",
      "        -2.1359, -2.1404, -2.1970, -2.1783, -2.1394, -2.1576, -2.1480, -2.1624,\n",
      "        -2.1448, -2.1472, -2.1676, -2.1576, -2.1882, -2.1514, -2.1626, -2.1463,\n",
      "        -2.1665, -2.1543, -2.1927, -2.1470, -2.1649, -2.1662, -2.1507, -2.1627,\n",
      "        -2.1687, -2.1597, -2.1450, -2.1940, -2.1560, -2.1555, -2.1520, -2.1503,\n",
      "        -2.1558, -2.1722, -2.1564, -2.1493, -2.1401, -2.1771, -2.1595, -2.1462,\n",
      "        -2.1718, -2.1508, -2.1740, -2.1412, -2.1693, -2.1805, -2.1585, -2.1628,\n",
      "        -2.1531, -2.1493, -2.1430, -2.1492, -2.1668, -2.1497, -2.1796, -2.1337,\n",
      "        -2.1647, -2.1618, -2.1745, -2.1479, -2.1473, -2.1514, -2.1650, -2.1846,\n",
      "        -2.1678, -2.1580, -2.1476, -2.1571, -2.1465, -2.1967, -2.1642, -2.1803,\n",
      "        -2.1688, -2.1429, -2.1609, -2.1660, -2.1498, -2.1572, -2.1569, -2.1714,\n",
      "        -2.1342, -2.1478, -2.1834, -2.1763, -2.1576, -2.1866, -2.1631, -2.1494,\n",
      "        -2.1839, -2.1807, -2.1593, -2.1681, -2.1484, -2.1627, -2.1746, -2.1828,\n",
      "        -2.1493, -2.1595, -2.1556, -2.1842, -2.1671, -2.1732, -2.1544, -2.1553,\n",
      "        -2.1432, -2.1549, -2.1513, -2.1628, -2.1589, -2.1728, -2.1514, -2.1457,\n",
      "        -2.1467, -2.1699, -2.1503, -2.1377, -2.1652, -2.1671, -2.1835, -2.1581,\n",
      "        -2.1633, -2.1510, -2.1724, -2.1497, -2.1636, -2.1629, -2.1857, -2.1606,\n",
      "        -2.1680, -2.1663, -2.1592, -2.1632, -2.1679, -2.1728, -2.1491, -2.1504,\n",
      "        -2.1477, -2.1667, -2.1453, -2.1562, -2.1719, -2.1454, -2.1555, -2.1381,\n",
      "        -2.1912, -2.1539, -2.1516, -2.1615, -2.1468, -2.1408, -2.1712, -2.1685,\n",
      "        -2.1786, -2.1533, -2.1635, -2.1386, -2.1572, -2.1674, -2.1511, -2.1626,\n",
      "        -2.1522, -2.1570, -2.1521, -2.1630, -2.1567, -2.1435, -2.1519, -2.1560,\n",
      "        -2.1591, -2.1712, -2.1560, -2.1707, -2.1634, -2.1663, -2.1548, -2.1299,\n",
      "        -2.1419, -2.1456, -2.1475, -2.1608, -2.1842, -2.1521, -2.1600, -2.1492],\n",
      "       grad_fn=<MaxBackward0>),\n",
      "indices=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]))\n",
      "truth: \n",
      "tensor([5, 5, 7, 1, 1, 0, 8, 7, 6, 1, 2, 8, 9, 5, 8, 2, 7, 1, 6, 9, 3, 7, 3, 0,\n",
      "        4, 7, 5, 2, 4, 1, 2, 6, 4, 4, 8, 3, 2, 4, 1, 4, 8, 2, 6, 9, 0, 0, 6, 6,\n",
      "        7, 7, 9, 4, 9, 9, 0, 4, 5, 1, 9, 2, 5, 0, 1, 4, 7, 2, 2, 6, 0, 3, 3, 6,\n",
      "        0, 8, 0, 9, 8, 4, 4, 3, 0, 6, 8, 8, 4, 3, 4, 1, 5, 4, 5, 1, 9, 1, 5, 3,\n",
      "        0, 5, 1, 2, 4, 1, 1, 4, 4, 0, 0, 8, 7, 4, 2, 0, 3, 3, 2, 3, 7, 8, 1, 7,\n",
      "        1, 9, 8, 3, 2, 6, 0, 6, 3, 2, 0, 9, 5, 3, 0, 3, 9, 5, 7, 3, 3, 4, 9, 4,\n",
      "        0, 8, 2, 1, 8, 5, 2, 1, 0, 4, 6, 5, 3, 6, 0, 4, 3, 7, 5, 8, 1, 0, 4, 7,\n",
      "        7, 8, 1, 6, 8, 9, 0, 0, 7, 1, 1, 7, 8, 8, 6, 7, 3, 5, 4, 1, 7, 9, 1, 9,\n",
      "        5, 7, 8, 3, 0, 8, 6, 4, 2, 9, 6, 4, 7, 4, 2, 1, 3, 1, 1, 6, 4, 4, 9, 2,\n",
      "        9, 4, 7, 4, 3, 7, 2, 4, 5, 1, 3, 2, 8, 4, 7, 4, 3, 6, 7, 7, 2, 3, 5, 9,\n",
      "        6, 5, 1, 2, 9, 0, 4, 4, 7, 1, 5, 4, 0, 9, 6, 2])\n",
      "loss tensor(2.3009, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data x shape:  torch.Size([19200, 1])\n",
      "data x shape:  torch.Size([19200, 3])\n",
      "data x shape:  torch.Size([6400, 3])\n",
      "data pos shape:  torch.Size([6400, 2])\n",
      "data batch shape:  torch.Size([6400])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([3531, 20])\n",
      "data x shape:  torch.Size([3531, 20])\n",
      "x shape:  torch.Size([1943, 20])\n",
      "x shape:  torch.Size([256, 20])\n",
      "torch.Size([256, 10])\n",
      "result: \n",
      "tensor([[-2.2519, -2.3377, -2.4201,  ..., -2.2855, -2.1804, -2.4031],\n",
      "        [-2.1904, -2.3710, -2.3987,  ..., -2.3193, -2.2293, -2.4265],\n",
      "        [-2.2154, -2.3397, -2.4036,  ..., -2.3018, -2.2130, -2.4174],\n",
      "        ...,\n",
      "        [-2.2755, -2.3250, -2.4238,  ..., -2.2702, -2.1750, -2.4042],\n",
      "        [-2.2425, -2.3361, -2.4150,  ..., -2.2849, -2.1854, -2.4051],\n",
      "        [-2.2525, -2.3304, -2.4243,  ..., -2.2792, -2.1523, -2.3912]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([256, 10])\n",
      "pred: \n",
      "torch.return_types.max(\n",
      "values=tensor([-2.1804, -2.1733, -2.2119, -2.1931, -2.1844, -2.1835, -2.1818, -2.1848,\n",
      "        -2.1937, -2.1833, -2.1727, -2.1852, -2.1767, -2.1965, -2.1918, -2.1723,\n",
      "        -2.1815, -2.1811, -2.1804, -2.1807, -2.1856, -2.1970, -2.1948, -2.1965,\n",
      "        -2.1810, -2.1981, -2.1960, -2.1884, -2.1487, -2.1696, -2.1689, -2.1810,\n",
      "        -2.1964, -2.1823, -2.1997, -2.1897, -2.1827, -2.1957, -2.1865, -2.1968,\n",
      "        -2.1966, -2.1923, -2.1950, -2.1837, -2.1664, -2.1965, -2.1883, -2.1901,\n",
      "        -2.1787, -2.1855, -2.1923, -2.1978, -2.1948, -2.1970, -2.1968, -2.1675,\n",
      "        -2.2000, -2.1736, -2.1917, -2.1771, -2.1867, -2.1907, -2.1972, -2.2017,\n",
      "        -2.1935, -2.1907, -2.1592, -2.1769, -2.1874, -2.1776, -2.1676, -2.1776,\n",
      "        -2.1982, -2.1791, -2.1909, -2.1943, -2.1899, -2.1851, -2.1813, -2.1578,\n",
      "        -2.2052, -2.2009, -2.2052, -2.1811, -2.1775, -2.1825, -2.1886, -2.1915,\n",
      "        -2.1894, -2.1943, -2.1769, -2.1652, -2.1871, -2.1696, -2.2037, -2.1934,\n",
      "        -2.1983, -2.1852, -2.2066, -2.1849, -2.1913, -2.1856, -2.1962, -2.1727,\n",
      "        -2.1907, -2.1836, -2.1933, -2.1835, -2.1944, -2.1884, -2.1703, -2.1850,\n",
      "        -2.1986, -2.1948, -2.1790, -2.1923, -2.1918, -2.2069, -2.1814, -2.1935,\n",
      "        -2.1933, -2.1928, -2.1912, -2.1726, -2.2073, -2.1943, -2.1895, -2.1833,\n",
      "        -2.1936, -2.1908, -2.1907, -2.1881, -2.1964, -2.1975, -2.1926, -2.1811,\n",
      "        -2.1974, -2.1797, -2.1710, -2.1805, -2.1895, -2.1950, -2.1735, -2.1804,\n",
      "        -2.1902, -2.1728, -2.1916, -2.1727, -2.2026, -2.1737, -2.1907, -2.1913,\n",
      "        -2.1899, -2.1986, -2.1950, -2.1997, -2.1886, -2.2041, -2.1799, -2.1926,\n",
      "        -2.1705, -2.1854, -2.1749, -2.2075, -2.1836, -2.1776, -2.1904, -2.1855,\n",
      "        -2.1692, -2.1896, -2.1864, -2.1962, -2.1712, -2.1615, -2.1928, -2.1588,\n",
      "        -2.1603, -2.1681, -2.1947, -2.1777, -2.1802, -2.1838, -2.1871, -2.1721,\n",
      "        -2.1996, -2.1748, -2.1838, -2.1835, -2.1894, -2.1865, -2.1991, -2.1660,\n",
      "        -2.1822, -2.1824, -2.1918, -2.1621, -2.1888, -2.1828, -2.2010, -2.1821,\n",
      "        -2.1671, -2.1798, -2.1977, -2.1692, -2.2089, -2.1814, -2.1771, -2.1892,\n",
      "        -2.1991, -2.1900, -2.1954, -2.1814, -2.1954, -2.1937, -2.1996, -2.1708,\n",
      "        -2.1976, -2.1725, -2.1961, -2.1848, -2.2035, -2.1863, -2.2098, -2.1872,\n",
      "        -2.1985, -2.2061, -2.1836, -2.1947, -2.1962, -2.1836, -2.1681, -2.1837,\n",
      "        -2.1964, -2.1888, -2.1986, -2.1899, -2.1777, -2.1834, -2.1644, -2.1808,\n",
      "        -2.1606, -2.1757, -2.1818, -2.1864, -2.1694, -2.2065, -2.2032, -2.1974,\n",
      "        -2.1995, -2.1999, -2.1820, -2.1963, -2.1900, -2.1750, -2.1854, -2.1523],\n",
      "       grad_fn=<MaxBackward0>),\n",
      "indices=tensor([8, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 8,\n",
      "        8, 8, 4, 8, 4, 8, 8, 8, 8, 8, 8, 4, 8, 8, 8, 8, 8, 8, 4, 8, 8, 4, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 4, 8, 8, 8, 8, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        4, 8, 8, 8, 4, 8, 8, 4, 8, 8, 8, 4, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 4, 8,\n",
      "        8, 8, 8, 8, 8, 8, 4, 4, 4, 8, 4, 8, 4, 8, 4, 8, 8, 8, 4, 8, 8, 4, 8, 8,\n",
      "        8, 8, 8, 8, 4, 8, 8, 8, 4, 8, 8, 8, 8, 4, 8, 8, 8, 8, 4, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 4, 8, 8, 8, 8, 4, 8, 8, 8, 8, 8, 4, 8, 8, 4, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 4, 8, 8, 4, 8, 0, 4, 8, 8, 8, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 4, 8, 4, 8, 4, 8, 8, 8, 8, 8, 8, 8, 4, 8, 8, 4, 8, 8, 4, 8,\n",
      "        8, 8, 8, 8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 4, 8, 4, 8, 8, 8, 8, 4, 8, 8, 8]))\n",
      "truth: \n",
      "tensor([2, 6, 0, 4, 3, 6, 0, 1, 6, 9, 7, 5, 2, 4, 9, 9, 2, 2, 0, 5, 2, 3, 9, 0,\n",
      "        7, 6, 2, 3, 2, 4, 7, 2, 1, 6, 7, 6, 4, 8, 0, 5, 3, 8, 8, 1, 1, 0, 6, 6,\n",
      "        1, 7, 7, 1, 3, 4, 8, 9, 7, 3, 7, 1, 3, 2, 5, 1, 3, 2, 7, 4, 7, 3, 1, 8,\n",
      "        6, 9, 7, 1, 9, 0, 4, 2, 3, 2, 7, 6, 3, 7, 7, 7, 1, 8, 2, 9, 7, 2, 4, 0,\n",
      "        7, 1, 5, 4, 6, 5, 6, 6, 4, 9, 6, 3, 4, 4, 5, 8, 7, 4, 2, 9, 6, 4, 2, 5,\n",
      "        8, 3, 7, 0, 3, 3, 3, 2, 0, 6, 5, 1, 0, 7, 9, 6, 6, 5, 6, 1, 1, 3, 2, 7,\n",
      "        3, 1, 7, 7, 8, 3, 6, 9, 4, 9, 6, 0, 7, 2, 7, 7, 5, 5, 9, 3, 6, 8, 4, 1,\n",
      "        4, 0, 5, 0, 6, 1, 4, 3, 9, 5, 2, 6, 7, 4, 4, 1, 2, 6, 3, 8, 6, 5, 6, 1,\n",
      "        4, 5, 5, 3, 1, 2, 4, 7, 3, 1, 9, 3, 6, 6, 5, 3, 6, 9, 8, 2, 1, 8, 1, 7,\n",
      "        5, 4, 1, 7, 3, 9, 3, 5, 4, 2, 7, 1, 0, 5, 4, 7, 6, 0, 8, 8, 0, 4, 4, 9,\n",
      "        4, 9, 7, 5, 0, 5, 7, 9, 0, 0, 5, 0, 0, 1, 7, 1])\n",
      "loss tensor(2.3040, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([19200, 1])\n",
      "data x shape:  torch.Size([19200, 3])\n",
      "data x shape:  torch.Size([6400, 3])\n",
      "data pos shape:  torch.Size([6400, 2])\n",
      "data batch shape:  torch.Size([6400])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([6400, 20])\n",
      "data x shape:  torch.Size([3516, 20])\n",
      "data x shape:  torch.Size([3516, 20])\n",
      "x shape:  torch.Size([1939, 20])\n",
      "x shape:  torch.Size([256, 20])\n",
      "torch.Size([256, 10])\n",
      "result: \n",
      "tensor([[-2.2416, -2.3350, -2.4127,  ..., -2.2792, -2.2152, -2.4127],\n",
      "        [-2.2156, -2.3347, -2.4095,  ..., -2.2970, -2.2173, -2.4107],\n",
      "        [-2.2186, -2.3400, -2.3986,  ..., -2.2997, -2.2173, -2.4144],\n",
      "        ...,\n",
      "        [-2.1437, -2.3900, -2.3890,  ..., -2.3429, -2.2728, -2.4383],\n",
      "        [-2.2046, -2.3393, -2.3956,  ..., -2.3048, -2.2478, -2.4283],\n",
      "        [-2.1986, -2.3522, -2.4103,  ..., -2.3106, -2.2111, -2.4061]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([256, 10])\n",
      "pred: \n",
      "torch.return_types.max(\n",
      "values=tensor([-2.1954, -2.2085, -2.1949, -2.1758, -2.2048, -2.2046, -2.1663, -2.2026,\n",
      "        -2.2050, -2.1878, -2.1982, -2.1905, -2.1561, -2.1911, -2.1502, -2.1780,\n",
      "        -2.1989, -2.1629, -2.1933, -2.2016, -2.2046, -2.1432, -2.1754, -2.2021,\n",
      "        -2.2031, -2.1822, -2.2064, -2.1908, -2.2086, -2.2019, -2.1892, -2.1695,\n",
      "        -2.1978, -2.1918, -2.2086, -2.2085, -2.2105, -2.1990, -2.2031, -2.1822,\n",
      "        -2.1952, -2.1894, -2.1508, -2.2005, -2.1590, -2.1790, -2.2080, -2.2030,\n",
      "        -2.1875, -2.1555, -2.1998, -2.1894, -2.1819, -2.1571, -2.1999, -2.1784,\n",
      "        -2.2030, -2.1938, -2.1911, -2.1515, -2.2021, -2.1806, -2.1986, -2.1806,\n",
      "        -2.1633, -2.2086, -2.1999, -2.2036, -2.1738, -2.1904, -2.2058, -2.2023,\n",
      "        -2.1954, -2.1614, -2.2068, -2.2010, -2.1984, -2.1935, -2.1928, -2.1982,\n",
      "        -2.1931, -2.1897, -2.1969, -2.2115, -2.1896, -2.2036, -2.1919, -2.2002,\n",
      "        -2.1955, -2.1875, -2.1831, -2.2039, -2.2047, -2.1881, -2.2030, -2.1765,\n",
      "        -2.1822, -2.1719, -2.1847, -2.2021, -2.1880, -2.2043, -2.1992, -2.1782,\n",
      "        -2.1970, -2.1994, -2.2066, -2.2031, -2.1613, -2.2002, -2.1788, -2.1896,\n",
      "        -2.2007, -2.1658, -2.2065, -2.1666, -2.2047, -2.2025, -2.2033, -2.1260,\n",
      "        -2.2048, -2.2093, -2.1947, -2.1955, -2.1749, -2.1970, -2.1893, -2.1949,\n",
      "        -2.1886, -2.2062, -2.1937, -2.1724, -2.1660, -2.1924, -2.1949, -2.2003,\n",
      "        -2.1363, -2.1932, -2.1726, -2.1965, -2.2101, -2.2012, -2.1887, -2.2026,\n",
      "        -2.1950, -2.1798, -2.2117, -2.2032, -2.1810, -2.1872, -2.1845, -2.1739,\n",
      "        -2.1757, -2.1802, -2.2101, -2.2040, -2.1682, -2.1671, -2.1928, -2.1962,\n",
      "        -2.2112, -2.1743, -2.2033, -2.1831, -2.1568, -2.1823, -2.1831, -2.1938,\n",
      "        -2.2047, -2.1673, -2.1777, -2.2077, -2.1522, -2.1800, -2.2060, -2.2041,\n",
      "        -2.1947, -2.2112, -2.2105, -2.1962, -2.2029, -2.1599, -2.1924, -2.2016,\n",
      "        -2.1635, -2.1967, -2.2019, -2.1612, -2.1819, -2.1811, -2.1898, -2.2116,\n",
      "        -2.1627, -2.1652, -2.2014, -2.1956, -2.1748, -2.1777, -2.1849, -2.1650,\n",
      "        -2.1914, -2.1979, -2.1602, -2.1706, -2.1923, -2.1735, -2.2020, -2.1797,\n",
      "        -2.1698, -2.1795, -2.1673, -2.2037, -2.1726, -2.1645, -2.1969, -2.1715,\n",
      "        -2.1856, -2.2086, -2.1959, -2.1962, -2.1360, -2.1869, -2.1922, -2.1724,\n",
      "        -2.2028, -2.1966, -2.1334, -2.1830, -2.2103, -2.1677, -2.1495, -2.1954,\n",
      "        -2.2067, -2.1951, -2.1812, -2.1891, -2.2002, -2.1979, -2.1775, -2.1780,\n",
      "        -2.2034, -2.2088, -2.1896, -2.1915, -2.2114, -2.2062, -2.1928, -2.1943,\n",
      "        -2.1955, -2.1755, -2.1881, -2.1866, -2.1750, -2.1351, -2.1736, -2.1986],\n",
      "       grad_fn=<MaxBackward0>),\n",
      "indices=tensor([4, 4, 4, 4, 3, 8, 4, 3, 8, 4, 4, 0, 4, 0, 4, 4, 4, 4, 4, 8, 3, 4, 4, 8,\n",
      "        3, 4, 4, 4, 8, 8, 0, 4, 3, 4, 8, 4, 4, 4, 3, 4, 4, 4, 4, 8, 4, 4, 8, 4,\n",
      "        4, 4, 3, 4, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 0, 4, 4, 4, 4, 0,\n",
      "        4, 4, 3, 3, 4, 4, 0, 8, 8, 4, 0, 8, 4, 0, 4, 3, 0, 4, 4, 4, 8, 0, 8, 4,\n",
      "        4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 8, 4, 4, 4, 4, 8, 4, 3, 4, 3, 8, 4, 4,\n",
      "        4, 8, 4, 4, 4, 8, 0, 4, 4, 4, 4, 4, 4, 0, 4, 8, 4, 4, 4, 4, 4, 3, 4, 3,\n",
      "        8, 4, 8, 4, 4, 8, 4, 4, 4, 4, 8, 4, 4, 4, 4, 4, 3, 4, 0, 4, 4, 4, 4, 4,\n",
      "        8, 4, 4, 4, 4, 4, 8, 0, 4, 3, 4, 4, 4, 4, 4, 8, 4, 8, 4, 4, 4, 4, 4, 0,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 0, 3, 4, 4, 0, 4, 3, 4, 4, 4, 4, 3, 4, 4, 8, 4,\n",
      "        4, 3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 8, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        3, 4, 8, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0]))\n",
      "truth: \n",
      "tensor([6, 7, 7, 0, 7, 7, 5, 9, 5, 2, 6, 9, 4, 2, 7, 7, 4, 7, 8, 7, 9, 6, 4, 3,\n",
      "        4, 1, 1, 7, 0, 4, 8, 3, 9, 8, 2, 5, 9, 2, 7, 1, 9, 0, 8, 7, 9, 3, 5, 5,\n",
      "        7, 4, 7, 3, 6, 7, 3, 3, 1, 0, 3, 2, 9, 4, 8, 5, 3, 5, 1, 2, 3, 7, 2, 1,\n",
      "        2, 2, 6, 9, 1, 7, 1, 7, 0, 9, 1, 3, 4, 3, 3, 4, 9, 1, 8, 6, 3, 5, 0, 6,\n",
      "        8, 8, 9, 9, 5, 1, 7, 9, 7, 8, 9, 1, 4, 9, 7, 4, 2, 9, 8, 0, 7, 0, 2, 0,\n",
      "        4, 3, 9, 0, 2, 4, 0, 8, 1, 4, 0, 3, 5, 2, 3, 0, 6, 5, 2, 0, 2, 0, 4, 3,\n",
      "        8, 6, 8, 3, 2, 5, 4, 8, 9, 3, 7, 6, 3, 9, 4, 4, 6, 6, 4, 8, 0, 9, 3, 2,\n",
      "        5, 2, 7, 9, 5, 2, 3, 1, 0, 6, 6, 8, 1, 5, 6, 4, 4, 1, 7, 9, 4, 4, 5, 0,\n",
      "        9, 3, 8, 8, 1, 7, 7, 0, 3, 1, 1, 8, 2, 5, 1, 8, 3, 0, 0, 4, 6, 2, 8, 2,\n",
      "        4, 6, 9, 3, 3, 3, 2, 9, 5, 1, 4, 2, 0, 2, 0, 3, 9, 9, 8, 8, 2, 3, 7, 6,\n",
      "        3, 5, 1, 4, 6, 9, 4, 5, 1, 9, 5, 7, 3, 6, 3, 2])\n",
      "loss tensor(2.3034, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data x shape:  torch.Size([19200, 1])\n",
      "data x shape:  torch.Size([19200, 3])\n",
      "data x shape:  torch.Size([6400, 3])\n",
      "data pos shape:  torch.Size([6400, 2])\n",
      "data batch shape:  torch.Size([6400])\n",
      "data x shape:  torch.Size([6400, 20])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f7435169ce54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {:02d}, Test: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f7435169ce54>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data batch shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f7435169ce54>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     40\u001b[0m                                            norm=torch.tensor([1., 1./27., 1./27.]))\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-22959a0a8389>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data x shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_undirected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data x shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/conv/edge_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmp_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'edge_index'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fuse__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mmsg_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__distribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__msg_params__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0maggr_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__distribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__aggr_params__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/conv/edge_conv.py\u001b[0m in \u001b[0;36mmessage\u001b[0;34m(self, x_i, x_j)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_j\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36melu\u001b[0;34m(input, alpha, inplace)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "path = osp.join('./', '..', 'data', 'MNIST')\n",
    "transform = T.Cartesian(cat=False)\n",
    "train_dataset = MNISTSuperpixels(path, True, transform=transform)\n",
    "test_dataset = MNISTSuperpixels(path, False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "epoch_size = len(train_dataset)\n",
    "print(epoch_size, batch_size)\n",
    "d = train_dataset\n",
    "\n",
    "print('features ->', d.num_features)\n",
    "print('classes ->',d.num_classes)\n",
    "\n",
    "# hidden_dim = int(sys.argv[1])\n",
    "hidden_dim = 20\n",
    "print('hidden_dim = %d' % hidden_dim)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.drn = DynamicReductionNetwork(input_dim=3, hidden_dim=hidden_dim,\n",
    "                                           k=4,\n",
    "                                           output_dim=d.num_classes, aggr='add',\n",
    "                                           norm=torch.tensor([1., 1./27., 1./27.]))\n",
    "    def forward(self, data):\n",
    "        logits = self.drn(data)\n",
    "        return F.log_softmax(logits, dim=1)\n",
    "\n",
    "def print_model_summary(model):\n",
    "    \"\"\"Override as needed\"\"\"\n",
    "    print(\n",
    "        'Model: \\n%s\\nParameters: %i' %\n",
    "        (model, sum(p.numel() for p in model.parameters()))\n",
    "    )\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "scheduler = CyclicLRWithRestarts(optimizer, batch_size, epoch_size, restart_period=400, t_mult=1.2, policy=\"cosine\")\n",
    "\n",
    "print_model_summary(model)\n",
    "\n",
    "\n",
    "# data batch attributes defined in MNIST superpixel class: \n",
    "# https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/datasets/mnist_superpixels.py\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "    \n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "#         print(data)\n",
    "        mask = (data.x > 0.).squeeze()\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "        data.x = torch.cat([data.x, data.pos], dim=-1)\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "        data.x = data.x[mask,:]\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "        data.pos = data.pos[mask,:]\n",
    "        print(\"data pos shape: \", data.pos.size())\n",
    "        data.batch = data.batch[mask.squeeze()]\n",
    "        print(\"data batch shape: \", data.batch.size())\n",
    "        optimizer.zero_grad()\n",
    "        result = model(data)\n",
    "        print(\"result: \")\n",
    "        print(result)\n",
    "        print(result.size())\n",
    "        print(\"pred: \")\n",
    "        print(result.max(1)[1])\n",
    "        print(\"truth: \")\n",
    "        print(data.y)\n",
    "\n",
    "        # negative log likelihood loss\n",
    "        loss = F.nll_loss(result, data.y)\n",
    "        print(\"loss\", loss)\n",
    "        loss.backward()\n",
    "        #print(torch.unique(torch.argmax(result, dim=-1)))\n",
    "        #print(torch.unique(data.y))\n",
    "        optimizer.step()\n",
    "        scheduler.batch_step()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    \n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        mask = (data.x > 0.).squeeze()\n",
    "        data.x = torch.cat([data.x, data.pos], dim=-1)\n",
    "        data.x = data.x[mask,:]\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "        data.pos = data.pos[mask,:]\n",
    "        print(\"data pos shape: \", data.pos.size())\n",
    "        data.batch = data.batch[mask.squeeze()]\n",
    "        print(\"data batch shape: \", data.batch.size())\n",
    "        pred = model(data).max(1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(test_dataset)\n",
    "\n",
    "for epoch in range(1, 10): # 401\n",
    "    train(epoch)\n",
    "    test_acc = test()\n",
    "    print('Epoch: {:02d}, Test: {:.4f}'.format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
