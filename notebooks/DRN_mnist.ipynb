{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch_cluster import knn_graph\n",
    "\n",
    "from torch_geometric.nn import EdgeConv, NNConv\n",
    "from torch_geometric.nn.pool.edge_pool import EdgePooling\n",
    "\n",
    "from torch_geometric.utils import normalized_cut\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "from torch_geometric.utils.undirected import to_undirected\n",
    "from torch_geometric.nn import (graclus, max_pool, max_pool_x,\n",
    "                                global_mean_pool, global_max_pool,\n",
    "                                global_add_pool)\n",
    "\n",
    "transform = T.Cartesian(cat=False)\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import math\n",
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceMaxLROnRestart:\n",
    "    def __init__(self, ratio=0.75):\n",
    "        self.ratio = ratio\n",
    "        \n",
    "        def __call__(self, eta_min, eta_max):\n",
    "            return eta_min, eta_max * self.ratio\n",
    "        \n",
    "        \n",
    "class ExpReduceMaxLROnIteration:\n",
    "    def __init__(self, gamma=1):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def __call__(self, eta_min, eta_max, iterations):\n",
    "        return eta_min, eta_max * self.gamma ** iterations\n",
    "\n",
    "\n",
    "class CosinePolicy:\n",
    "    def __call__(self, t_cur, restart_period):\n",
    "        return 0.5 * (1. + math.cos(math.pi *\n",
    "                                    (t_cur / restart_period)))\n",
    "    \n",
    "    \n",
    "class ArccosinePolicy:\n",
    "    def __call__(self, t_cur, restart_period):\n",
    "        return (math.acos(max(-1, min(1, 2 * t_cur\n",
    "                                      / restart_period - 1))) / math.pi)\n",
    "    \n",
    "    \n",
    "class TriangularPolicy:\n",
    "    def __init__(self, triangular_step=0.5):\n",
    "        self.triangular_step = triangular_step\n",
    "        \n",
    "    def __call__(self, t_cur, restart_period):\n",
    "        inflection_point = self.triangular_step * restart_period\n",
    "        point_of_triangle = (t_cur / inflection_point\n",
    "                             if t_cur < inflection_point\n",
    "                             else 1.0 - (t_cur - inflection_point)\n",
    "                             / (restart_period - inflection_point))\n",
    "        return point_of_triangle\n",
    "    \n",
    "    \n",
    "class CyclicLRWithRestarts(_LRScheduler):\n",
    "    \"\"\"Decays learning rate with cosine annealing, normalizes weight decay\n",
    "    hyperparameter value, implements restarts.\n",
    "    https://arxiv.org/abs/1711.05101\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        batch_size: minibatch size\n",
    "        epoch_size: training samples per epoch\n",
    "        restart_period: epoch count in the first restart period\n",
    "        t_mult: multiplication factor by which the next restart period will expand/shrink\n",
    "        policy: [\"cosine\", \"arccosine\", \"triangular\", \"triangular2\", \"exp_range\"]\n",
    "        min_lr: minimum allowed learning rate\n",
    "        verbose: print a message on every restart\n",
    "        gamma: exponent used in \"exp_range\" policy\n",
    "        eta_on_restart_cb: callback executed on every restart, adjusts max or min lr\n",
    "        eta_on_iteration_cb: callback executed on every iteration, adjusts max or min lr\n",
    "        triangular_step: adjusts ratio of increasing/decreasing phases for triangular policy\n",
    "    Example:\n",
    "        >>> scheduler = CyclicLRWithRestarts(optimizer, 32, 1024, restart_period=5, t_mult=1.2)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     scheduler.step()\n",
    "        >>>     train(...)\n",
    "        >>>         ...\n",
    "        >>>         optimizer.zero_grad()\n",
    "        >>>         loss.backward()\n",
    "        >>>         optimizer.step()\n",
    "        >>>         scheduler.batch_step()\n",
    "        >>>     validate(...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, batch_size, epoch_size, restart_period=100,\n",
    "                 t_mult=2, last_epoch=-1, verbose=False,\n",
    "                 policy=\"cosine\", policy_fn=None, min_lr=1e-7,\n",
    "                 eta_on_restart_cb=None, eta_on_iteration_cb=None,\n",
    "                 gamma=1.0, triangular_step=0.5):\n",
    "        \n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        if last_epoch == -1:\n",
    "            for group in optimizer.param_groups:\n",
    "                group.setdefault('initial_lr', group['lr'])\n",
    "                group.setdefault('minimum_lr', min_lr)\n",
    "        else:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if 'initial_lr' not in group:\n",
    "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
    "                                   \"in param_groups[{}] when resuming an\"\n",
    "                                   \" optimizer\".format(i))\n",
    "                \n",
    "        self.base_lrs = [group['initial_lr'] for group\n",
    "                         in optimizer.param_groups]\n",
    "        \n",
    "        self.min_lrs = [group['minimum_lr'] for group\n",
    "                        in optimizer.param_groups]\n",
    "        \n",
    "        self.base_weight_decays = [group['weight_decay'] for group\n",
    "                                   in optimizer.param_groups]\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.eta_on_restart_cb = eta_on_restart_cb\n",
    "        self.eta_on_iteration_cb = eta_on_iteration_cb\n",
    "        if policy_fn is not None:\n",
    "            self.policy_fn = policy_fn\n",
    "        elif self.policy == \"cosine\":\n",
    "            self.policy_fn = CosinePolicy()\n",
    "        elif self.policy == \"arccosine\":\n",
    "            self.policy_fn = ArccosinePolicy()\n",
    "        elif self.policy == \"triangular\":\n",
    "            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n",
    "        elif self.policy == \"triangular2\":\n",
    "            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n",
    "            self.eta_on_restart_cb = ReduceMaxLROnRestart(ratio=0.5)\n",
    "        elif self.policy == \"exp_range\":\n",
    "            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n",
    "            self.eta_on_iteration_cb = ExpReduceMaxLROnIteration(gamma=gamma)\n",
    "            \n",
    "        self.last_epoch = last_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_size = epoch_size\n",
    "        \n",
    "        self.iteration = 0\n",
    "        self.total_iterations = 0\n",
    "        \n",
    "        self.t_mult = t_mult\n",
    "        self.verbose = verbose\n",
    "        self.restart_period = math.ceil(restart_period)\n",
    "        self.restarts = 0\n",
    "        self.t_epoch = -1\n",
    "        self.epoch = -1\n",
    "        \n",
    "        self.eta_min = 0\n",
    "        self.eta_max = 1\n",
    "        \n",
    "        self.end_of_period = False\n",
    "        self.batch_increments = []\n",
    "        self._set_batch_increment()\n",
    "        \n",
    "    def _on_restart(self):\n",
    "        if self.eta_on_restart_cb is not None:\n",
    "            self.eta_min, self.eta_max = self.eta_on_restart_cb(self.eta_min,\n",
    "                                                                self.eta_max)\n",
    "            \n",
    "    def _on_iteration(self):\n",
    "        if self.eta_on_iteration_cb is not None:\n",
    "            self.eta_min, self.eta_max = self.eta_on_iteration_cb(self.eta_min,\n",
    "                                                                  self.eta_max,\n",
    "                                                                  self.total_iterations)\n",
    "            \n",
    "    def get_lr(self, t_cur):\n",
    "        eta_t = (self.eta_min + (self.eta_max - self.eta_min)\n",
    "                 * self.policy_fn(t_cur, self.restart_period))\n",
    "        \n",
    "        weight_decay_norm_multi = math.sqrt(self.batch_size /\n",
    "                                            (self.epoch_size *\n",
    "                                             self.restart_period))\n",
    "        \n",
    "        lrs = [min_lr + (base_lr - min_lr) * eta_t for base_lr, min_lr\n",
    "               in zip(self.base_lrs, self.min_lrs)]\n",
    "        weight_decays = [base_weight_decay #* eta_t * weight_decay_norm_multi\n",
    "                         for base_weight_decay in self.base_weight_decays]\n",
    "        \n",
    "        if (self.t_epoch + 1) % self.restart_period < self.t_epoch:\n",
    "            self.end_of_period = True\n",
    "            \n",
    "        if self.t_epoch % self.restart_period < self.t_epoch:\n",
    "            if self.verbose:\n",
    "                print(\"Restart {} at epoch {}\".format(self.restarts + 1,\n",
    "                                                      self.last_epoch))\n",
    "            self.restart_period = math.ceil(self.restart_period * self.t_mult)\n",
    "            self.restarts += 1\n",
    "            self.t_epoch = 0\n",
    "            self._on_restart()\n",
    "            self.end_of_period = False\n",
    "            \n",
    "        return zip(lrs, weight_decays)\n",
    "        \n",
    "    def _set_batch_increment(self):\n",
    "        d, r = divmod(self.epoch_size, self.batch_size)\n",
    "        batches_in_epoch = d + 2 if r > 0 else d + 1\n",
    "        self.iteration = 0\n",
    "        self.batch_increments = torch.linspace(0, 1, batches_in_epoch).tolist()\n",
    "        \n",
    "    def step(self):\n",
    "        self.last_epoch += 1\n",
    "        self.t_epoch += 1\n",
    "        self._set_batch_increment()\n",
    "        self.batch_step()\n",
    "        \n",
    "    def batch_step(self):\n",
    "        try:\n",
    "            t_cur = self.t_epoch + self.batch_increments[self.iteration]\n",
    "            self._on_iteration()\n",
    "            self.iteration += 1\n",
    "            self.total_iterations += 1\n",
    "        except (IndexError):\n",
    "            raise StopIteration(\"Epoch size and batch size used in the \"\n",
    "                                \"training loop and while initializing \"\n",
    "                                \"scheduler should be the same.\")\n",
    "        \n",
    "        for param_group, (lr, weight_decay) in zip(self.optimizer.param_groups,\n",
    "                                                   self.get_lr(t_cur)):\n",
    "            param_group['lr'] = lr\n",
    "            param_group['weight_decay'] = weight_decay\n",
    "\n",
    "def normalized_cut_2d(edge_index, pos):\n",
    "    row, col = edge_index\n",
    "    edge_attr = torch.norm(pos[row] - pos[col], p=2, dim=1)\n",
    "    return normalized_cut(edge_index, edge_attr, num_nodes=pos.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicReductionNetwork(nn.Module):\n",
    "    # This model clusters nearest neighbour graphs\n",
    "    # in two steps.\n",
    "    # The latent space trained to group useful features at each level\n",
    "    # of aggregration.\n",
    "    # This allows single quantities to be regressed from complex point counts\n",
    "    # in a location and orientation invariant way.\n",
    "    # One encoding layer is used to abstract away the input features.\n",
    "    def __init__(self, input_dim=5, hidden_dim=64, output_dim=1, k=16, aggr='add',\n",
    "                 norm=torch.tensor([1./500., 1./500., 1./54., 1/25., 1./1000.])):\n",
    "        super(DynamicReductionNetwork, self).__init__()\n",
    "\n",
    "        self.datanorm = nn.Parameter(norm)\n",
    "\n",
    "        self.k = k\n",
    "        start_width = 2 * hidden_dim\n",
    "        middle_width = 3 * hidden_dim // 2\n",
    "\n",
    "        self.inputnet =  nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim//2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        convnn1 = nn.Sequential(nn.Linear(start_width, middle_width),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(middle_width, hidden_dim),\n",
    "                                nn.ELU(),\n",
    "                                )\n",
    "        convnn2 = nn.Sequential(nn.Linear(start_width, middle_width),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(middle_width, hidden_dim),\n",
    "                                nn.ELU(),\n",
    "                                )\n",
    "        self.edgeconv1 = EdgeConv(nn=convnn1, aggr=aggr)\n",
    "        self.edgeconv2 = EdgeConv(nn=convnn2, aggr=aggr)\n",
    "        \n",
    "        self.output = nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n",
    "                                    nn.ELU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim//2),\n",
    "                                    nn.ELU(),\n",
    "                                    nn.Linear(hidden_dim//2, output_dim))\n",
    "\n",
    "    def forward(self, data):\n",
    "        data.x = self.datanorm * data.x\n",
    "        data.x = self.inputnet(data.x)\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "        data.edge_index = to_undirected(knn_graph(data.x, self.k, data.batch, loop=False, flow=self.edgeconv1.flow))\n",
    "        data.x = self.edgeconv1(data.x, data.edge_index)\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "\n",
    "        weight = normalized_cut_2d(data.edge_index, data.x)\n",
    "        cluster = graclus(data.edge_index, weight, data.x.size(0))\n",
    "        data.edge_attr = None\n",
    "        data = max_pool(cluster, data)\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "\n",
    "        data.edge_index = to_undirected(knn_graph(data.x, self.k, data.batch, loop=False, flow=self.edgeconv2.flow))\n",
    "        data.x = self.edgeconv2(data.x, data.edge_index)\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "\n",
    "        weight = normalized_cut_2d(data.edge_index, data.x)\n",
    "        cluster = graclus(data.edge_index, weight, data.x.size(0))\n",
    "        x, batch = max_pool_x(cluster, data.x, data.batch)\n",
    "        print(\"x shape: \", x.size())\n",
    "\n",
    "        x = global_max_pool(x, batch)\n",
    "        print(\"x shape: \", x.size())\n",
    "        print(self.output(x).squeeze(-1).size())\n",
    "        return self.output(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 300\n",
      "features -> 1\n",
      "classes -> 10\n",
      "hidden_dim = 20\n",
      "Model: \n",
      "Net(\n",
      "  (drn): DynamicReductionNetwork(\n",
      "    (inputnet): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=10, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=10, out_features=20, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "      (5): ELU(alpha=1.0)\n",
      "    )\n",
      "    (edgeconv1): EdgeConv(nn=Sequential(\n",
      "      (0): Linear(in_features=40, out_features=30, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "    ))\n",
      "    (edgeconv2): EdgeConv(nn=Sequential(\n",
      "      (0): Linear(in_features=40, out_features=30, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "    ))\n",
      "    (output): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=20, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Parameters: 5123\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4111, 20])\n",
      "data x shape:  torch.Size([4111, 20])\n",
      "x shape:  torch.Size([2247, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.9682, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4127, 20])\n",
      "data x shape:  torch.Size([4127, 20])\n",
      "x shape:  torch.Size([2252, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.4226, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4119, 20])\n",
      "data x shape:  torch.Size([4119, 20])\n",
      "x shape:  torch.Size([2248, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.3264, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4135, 20])\n",
      "data x shape:  torch.Size([4135, 20])\n",
      "x shape:  torch.Size([2277, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.3112, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4137, 20])\n",
      "data x shape:  torch.Size([4137, 20])\n",
      "x shape:  torch.Size([2276, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.3223, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4139, 20])\n",
      "data x shape:  torch.Size([4139, 20])\n",
      "x shape:  torch.Size([2278, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.3378, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4147, 20])\n",
      "data x shape:  torch.Size([4147, 20])\n",
      "x shape:  torch.Size([2276, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.3053, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4142, 20])\n",
      "data x shape:  torch.Size([4142, 20])\n",
      "x shape:  torch.Size([2270, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.3368, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4127, 20])\n",
      "data x shape:  torch.Size([4127, 20])\n",
      "x shape:  torch.Size([2269, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2968, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4121, 20])\n",
      "data x shape:  torch.Size([4121, 20])\n",
      "x shape:  torch.Size([2256, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.3191, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4135, 20])\n",
      "data x shape:  torch.Size([4135, 20])\n",
      "x shape:  torch.Size([2258, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.3023, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4118, 20])\n",
      "data x shape:  torch.Size([4118, 20])\n",
      "x shape:  torch.Size([2259, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2776, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4116, 20])\n",
      "data x shape:  torch.Size([4116, 20])\n",
      "x shape:  torch.Size([2268, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2653, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4125, 20])\n",
      "data x shape:  torch.Size([4125, 20])\n",
      "x shape:  torch.Size([2270, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2506, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4127, 20])\n",
      "data x shape:  torch.Size([4127, 20])\n",
      "x shape:  torch.Size([2267, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2501, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4129, 20])\n",
      "data x shape:  torch.Size([4129, 20])\n",
      "x shape:  torch.Size([2270, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2435, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4143, 20])\n",
      "data x shape:  torch.Size([4143, 20])\n",
      "x shape:  torch.Size([2279, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2497, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4115, 20])\n",
      "data x shape:  torch.Size([4115, 20])\n",
      "x shape:  torch.Size([2261, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2043, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4125, 20])\n",
      "data x shape:  torch.Size([4125, 20])\n",
      "x shape:  torch.Size([2267, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2376, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4106, 20])\n",
      "data x shape:  torch.Size([4106, 20])\n",
      "x shape:  torch.Size([2254, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2171, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4107, 20])\n",
      "data x shape:  torch.Size([4107, 20])\n",
      "x shape:  torch.Size([2270, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2219, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4138, 20])\n",
      "data x shape:  torch.Size([4138, 20])\n",
      "x shape:  torch.Size([2263, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2061, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4126, 20])\n",
      "data x shape:  torch.Size([4126, 20])\n",
      "x shape:  torch.Size([2271, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2323, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4150, 20])\n",
      "data x shape:  torch.Size([4150, 20])\n",
      "x shape:  torch.Size([2283, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2233, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4104, 20])\n",
      "data x shape:  torch.Size([4104, 20])\n",
      "x shape:  torch.Size([2246, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.1997, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4130, 20])\n",
      "data x shape:  torch.Size([4130, 20])\n",
      "x shape:  torch.Size([2267, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2321, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4108, 20])\n",
      "data x shape:  torch.Size([4108, 20])\n",
      "x shape:  torch.Size([2251, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.2082, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4118, 20])\n",
      "data x shape:  torch.Size([4118, 20])\n",
      "x shape:  torch.Size([2268, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.1535, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4128, 20])\n",
      "data x shape:  torch.Size([4128, 20])\n",
      "x shape:  torch.Size([2260, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.1896, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4129, 20])\n",
      "data x shape:  torch.Size([4129, 20])\n",
      "x shape:  torch.Size([2260, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.1897, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4121, 20])\n",
      "data x shape:  torch.Size([4121, 20])\n",
      "x shape:  torch.Size([2265, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.1849, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4147, 20])\n",
      "data x shape:  torch.Size([4147, 20])\n",
      "x shape:  torch.Size([2269, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.1228, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4130, 20])\n",
      "data x shape:  torch.Size([4130, 20])\n",
      "x shape:  torch.Size([2263, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.1443, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4140, 20])\n",
      "data x shape:  torch.Size([4140, 20])\n",
      "x shape:  torch.Size([2272, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.1205, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4127, 20])\n",
      "data x shape:  torch.Size([4127, 20])\n",
      "x shape:  torch.Size([2266, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.1296, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4142, 20])\n",
      "data x shape:  torch.Size([4142, 20])\n",
      "x shape:  torch.Size([2279, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.1067, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4130, 20])\n",
      "data x shape:  torch.Size([4130, 20])\n",
      "x shape:  torch.Size([2265, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.0845, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4100, 20])\n",
      "data x shape:  torch.Size([4100, 20])\n",
      "x shape:  torch.Size([2244, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.0960, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([4125, 20])\n",
      "data x shape:  torch.Size([4125, 20])\n",
      "x shape:  torch.Size([2260, 20])\n",
      "x shape:  torch.Size([300, 20])\n",
      "torch.Size([300, 10])\n",
      "loss tensor(2.0681, grad_fn=<NllLossBackward>)\n",
      "data x shape:  torch.Size([7500, 20])\n",
      "data x shape:  torch.Size([7500, 20])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f68effc88a6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {:02d}, Test: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f68effc88a6f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#         print(\"data batch shape: \", data.batch.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;31m#         print(\"result: \")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m#         print(result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f68effc88a6f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-22959a0a8389>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mcluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraclus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data x shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/pool/max_pool.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(cluster, data, transform)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_max_pool_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpool_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpool_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/pool/pool.py\u001b[0m in \u001b[0;36mpool_edge\u001b[0;34m(cluster, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         edge_index, edge_attr = coalesce(edge_index, edge_attr, num_nodes,\n\u001b[0;32m---> 12\u001b[0;31m                                          num_nodes)\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch_sparse/coalesce.py\u001b[0m in \u001b[0;36mcoalesce\u001b[0;34m(index, value, m, n, op)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     storage = SparseStorage(row=index[0], col=index[1], value=value,\n\u001b[0;32m---> 23\u001b[0;31m                             sparse_sizes=(m, n), is_sorted=False)\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch_sparse/storage.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, row, rowptr, col, value, sparse_sizes, rowcount, colptr, colcount, csr2csc, csc2csr, is_sorted)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "batch_size = 300\n",
    "\n",
    "path = osp.join('./', '..', 'data', 'MNIST')\n",
    "transform = T.Cartesian(cat=False)\n",
    "train_dataset = MNISTSuperpixels(path, True, transform=transform)\n",
    "test_dataset = MNISTSuperpixels(path, False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "epoch_size = len(train_dataset)\n",
    "print(epoch_size, batch_size)\n",
    "d = train_dataset\n",
    "\n",
    "print('features ->', d.num_features)\n",
    "print('classes ->',d.num_classes)\n",
    "\n",
    "# hidden_dim = int(sys.argv[1])\n",
    "hidden_dim = 20\n",
    "print('hidden_dim = %d' % hidden_dim)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.drn = DynamicReductionNetwork(input_dim=3, hidden_dim=hidden_dim,\n",
    "                                           k=4,\n",
    "                                           output_dim=d.num_classes, aggr='add',\n",
    "                                           norm=torch.tensor([1., 1./27., 1./27.]))\n",
    "\n",
    "    def forward(self, data):\n",
    "        logits = self.drn(data)\n",
    "        return F.log_softmax(logits, dim=1)\n",
    "\n",
    "def print_model_summary(model):\n",
    "    \"\"\"Override as needed\"\"\"\n",
    "    print(\n",
    "        'Model: \\n%s\\nParameters: %i' %\n",
    "        (model, sum(p.numel() for p in model.parameters()))\n",
    "    )\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "scheduler = CyclicLRWithRestarts(optimizer, batch_size, epoch_size, restart_period=400, t_mult=1.2, policy=\"cosine\")\n",
    "\n",
    "print_model_summary(model)\n",
    "\n",
    "\n",
    "# data batch attributes defined in MNIST superpixel class: \n",
    "# https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/datasets/mnist_superpixels.py\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "    \n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "#         print(data)\n",
    "        mask = (data.x > 0.).squeeze()\n",
    "#         print(\"data x shape: \", data.x.size())\n",
    "        data.x = torch.cat([data.x, data.pos], dim=-1)\n",
    "#         print(\"data x shape: \", data.x.size())\n",
    "        data.x = data.x[mask,:]\n",
    "#         print(\"data x shape: \", data.x.size())\n",
    "        data.pos = data.pos[mask,:]\n",
    "#         print(\"data pos shape: \", data.pos.size())\n",
    "        data.batch = data.batch[mask.squeeze()]\n",
    "#         print(\"data batch shape: \", data.batch.size())\n",
    "        optimizer.zero_grad()\n",
    "        result = model(data)\n",
    "#         print(\"result: \")\n",
    "#         print(result)\n",
    "#         print(result.size())\n",
    "#         print(\"pred: \")\n",
    "#         print(result.max(1)[1])\n",
    "#         print(\"truth: \")\n",
    "#         print(data.y)\n",
    "\n",
    "        # negative log likelihood loss\n",
    "        loss = F.nll_loss(result, data.y)\n",
    "        print(\"loss\", loss)\n",
    "        loss.backward()\n",
    "        #print(torch.unique(torch.argmax(result, dim=-1)))\n",
    "        #print(torch.unique(data.y))\n",
    "        optimizer.step()\n",
    "        scheduler.batch_step()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    \n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        mask = (data.x > 0.).squeeze()\n",
    "        data.x = torch.cat([data.x, data.pos], dim=-1)\n",
    "        data.x = data.x[mask,:]\n",
    "        print(\"data x shape: \", data.x.size())\n",
    "        data.pos = data.pos[mask,:]\n",
    "        print(\"data pos shape: \", data.pos.size())\n",
    "        data.batch = data.batch[mask.squeeze()]\n",
    "        print(\"data batch shape: \", data.batch.size())\n",
    "        pred = model(data).max(1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(test_dataset)\n",
    "\n",
    "for epoch in range(1, 10): # 401\n",
    "    train(epoch)\n",
    "    test_acc = test()\n",
    "    print('Epoch: {:02d}, Test: {:.4f}'.format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
